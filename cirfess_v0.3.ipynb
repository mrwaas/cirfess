{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Motif Coverage Predictor\n",
    "\n",
    "This program is designed to return the number of peptides for a protein (or set of proteins) which contain a given sequence motif (N-glyco motif) and are suitable for mass spectrometry. \n",
    "\n",
    "The program considers protein-level information from prediction tools such as `TMHMM`, `Phobius`, and `SignalP` in order to better interpret the context of possible sequence motifs. \n",
    "\n",
    "## I/O and Usage\n",
    "\n",
    "### Usage\n",
    "\n",
    "### Input\n",
    "\n",
    "\n",
    "### Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the definitions of functions to parse the various input file types, including:\n",
    "    # TMHMM short format output\n",
    "    # Phobius short format output\n",
    "    # Signal P short format output\n",
    "    # Fasta file\n",
    "\n",
    "# \n",
    "\n",
    "# Dependencies\n",
    "    # regular expressions (re)\n",
    "\n",
    "\n",
    "\n",
    "def parse_TMHMM(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        tm_dict = dict()\n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID = line.split('\\t')[0]\n",
    "            length = int(line.split('\\t')[1].split('=')[1])\n",
    "            topo_str = line.split('\\t')[5].split('=')[1]\n",
    "            num_TM = int(line.split('\\t')[4].split('=')[1])\n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : 'n/a' }\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : topo_str } \n",
    "                \n",
    "        \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                   i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                   o1.append(1)\n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "                    \n",
    "    return tm_dict\n",
    "\n",
    "def parse_Phobius(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "        tm_dict = dict()\n",
    "        \n",
    "        with open('io_files/human_plus_leftovers.tab' , 'r') as fo2:\n",
    "            length_dict = dict()\n",
    "            header = True\n",
    "            for line in fo2:\n",
    "                if header:\n",
    "                    header = False\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    length = line.split('\\t')[6]\n",
    "                    length_dict[ID] = int(length)\n",
    "                            \n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID, num_TM, SP, topo_str = line.split()\n",
    "            ID = ID.split('|')[1]\n",
    "            \n",
    "            if bool(length_dict.get(ID)):\n",
    "                length = length_dict[ID]\n",
    "            else:\n",
    "                continue   \n",
    "            \n",
    "            if len(topo_str) < 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : 'n/a' , \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : topo_str ,  \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "                \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                    i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                    o1.append(1)\n",
    "                else:\n",
    "                    match = re.search(r'\\S+/(\\S+?)([io])(\\S*)', topo)\n",
    "                    if match:\n",
    "                        topo = match.group(2) + match.group(3)\n",
    "                    else:\n",
    "                        print(topo)\n",
    "                    if topo.startswith('i'):\n",
    "                        i1.append(int(match.group(1)))\n",
    "                    elif topo.startswith('o'):\n",
    "                        o1.append(int(match.group(1)))\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def parse_signalP(file_name):\n",
    "    \n",
    "        with open(file_name, 'r') as fo:\n",
    "            \n",
    "            SP_dict = dict()\n",
    "            \n",
    "            for line in fo:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    SP = line.split()[1]\n",
    "                    score = line.split()[2]\n",
    "                    \n",
    "                    if SP.startswith('SP'):\n",
    "                        SP = 'Y'\n",
    "                    else:\n",
    "                        SP = 0\n",
    "            \n",
    "                    SP_dict[ID] = { 'SP': SP , 'score' : score } \n",
    "        \n",
    "        return SP_dict\n",
    "    \n",
    "def parse_Predisi(file_name):\n",
    "    \n",
    "    with open (file_name, 'r') as fo:\n",
    "        \n",
    "        predisi_dict = dict()\n",
    "        header = True\n",
    "        \n",
    "        for line in fo:\n",
    "            if header:\n",
    "                header = False\n",
    "            else:\n",
    "                line = line.rstrip()\n",
    "                ID = line.split('\\t')[0].split('|')[1]\n",
    "                SP = line.split('\\t')[-2]\n",
    "                score  = line.split('\\t')[-4]\n",
    "                \n",
    "                predisi_dict[ID] = {'SP' : SP, 'score' : score}\n",
    "                \n",
    "                \n",
    "    return predisi_dict\n",
    "    \n",
    "def parse_SPC(seq_dict, file_name):\n",
    "    \n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "            SPC_dict = dict()\n",
    "            header = True\n",
    "\n",
    "            for line in fo:\n",
    "                    if header:\n",
    "                        header = False\n",
    "                    else:\n",
    "                        line = line.rstrip()\n",
    "                        ID, SPC, BF, T, dC, DR = line.split(',') \n",
    "                        \n",
    "                        SPC_dict[ID] = dict() \n",
    "                        SPC_dict[ID]['score'] = int(SPC)\n",
    "                        \n",
    "                        stringOut = ''\n",
    "                        \n",
    "                        if int(BF) == 1 :\n",
    "                            stringOut += 'BF'\n",
    "                        if int(T) == 1 :\n",
    "                            stringOut += ',T'\n",
    "                        if int(dC) == 1:\n",
    "                            stringOut += ',dC'\n",
    "                        if int(DR) == 1:\n",
    "                            stringOut += ',DR'\n",
    "                        \n",
    "                        stringOut = stringOut.lstrip(',')                            \n",
    "                        \n",
    "                        SPC_dict[ID]['stringOut'] = stringOut\n",
    "            \n",
    "            for ID in seq_dict:\n",
    "                \n",
    "                if SPC_dict.get(ID) == None:\n",
    "                    SPC_dict[ID] = {'score': 0, 'stringOut' : 'n/a'}\n",
    "            \n",
    "    return SPC_dict\n",
    "        \n",
    "        \n",
    "            \n",
    "def fasta_parser(fasta_filename):   \n",
    "    \n",
    "\n",
    "    fasta_fileobj = open(fasta_filename, 'r')\t## create a file obj from the specified file\n",
    "\n",
    "    sequence_name = ''\t\t\t\t## initialize strings to populate from file object info\n",
    "    sequence_desc = ''\n",
    "    sequence_string = ''\n",
    "    sequence_dict = {}\n",
    "\n",
    "    for line in fasta_fileobj:  \t\t\t## iterate through file object with for loop\n",
    "        line = line.rstrip()\t\t\t## strip white space on the right side (like a new line character!) \n",
    "\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(sequence_string) > 0:\n",
    "                sequence_dict[sequence_name] = sequence_string\t\n",
    "                sequence_string = ''  \t\t## reset for the new sequence\n",
    "            \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  ## split on only first space\n",
    "            sequence_name = sequence_info[0].split('|')[1]\n",
    "\t\n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "            else:\t\t\t\t\t## sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "\t\t\n",
    "           \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  \t## split on only first space\n",
    "           \n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "           \n",
    "            else:\n",
    "            # sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "             \n",
    "        else:\n",
    "            sequence_string += line  # incrementally elongate seq\n",
    "\n",
    "# When we reach the end of the FASTA file, we drop out of the\n",
    "# 'for' loop. However, we still have the last sequence record\n",
    "# stored in memory, which we haven't processed yet, because we\n",
    "# haven't observed a '>' symbol, so we must copy and paste any\n",
    "# code that we used to process sequences above to the code block\n",
    "# below. Check if sequence_string has a non-zero length to\n",
    "# determine whether to execute the sequence processing code:\n",
    "\n",
    "    if len(sequence_string) > 0:\n",
    "        sequence_dict[sequence_name] = sequence_string\n",
    "        \n",
    "    return sequence_dict\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contatins the definition of functions required \n",
    "\n",
    "import re\n",
    "\n",
    "## 2 missed cleavages are used for trypsinize, where the number of missed cleavages are recorded\n",
    "\n",
    "def trypsinize(prot_seq):\n",
    "    peptides= []\n",
    "    cut_sites=[0]\n",
    "    indices = []\n",
    "    pep = ''\n",
    "\n",
    "    for i in range(0,len(prot_seq)-1):\n",
    "        if prot_seq[i] == 'K' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        elif prot_seq[i] == 'R' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        \n",
    "    if cut_sites[-1]!=len(prot_seq):\n",
    "            cut_sites.append(len(prot_seq))\n",
    "            \n",
    "    if len(cut_sites)>2:\n",
    "            \n",
    "        for j in range(0,len(cut_sites)-3):\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+2]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+2]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+3]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+3]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 2})\n",
    "            indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-2]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-2]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-2]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-2],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "                    \n",
    "    else: #there is no trypsin site in the protein sequence\n",
    "        peptides.append({'seq' : prot_seq, 'indices' : range(1,len(prot_seq)+1), 'missed_cleavages' : 0})\n",
    "        \n",
    "    return peptides\n",
    "\n",
    "def ion_mim(prot_seq, charge_state):\n",
    "    \n",
    "    mass_table = {\n",
    "            \"A\" : 71.03711,\n",
    "            \"R\" : 156.10111,\n",
    "            \"N\" : 114.04293,\n",
    "            \"D\" : 115.02694,\n",
    "            \"C\" : 103.00919 + 57.02146,\n",
    "            \"E\" : 129.04259,\n",
    "            \"Q\" : 128.05858,\n",
    "            \"G\" : 57.02146,\n",
    "            \"H\" : 137.05891,\n",
    "            \"I\" : 113.08406,\n",
    "            \"L\" : 113.08406,\n",
    "            \"K\" : 128.09496,\n",
    "            \"M\" : 131.04049,\n",
    "            \"F\" : 147.06841,\n",
    "            \"P\" : 97.05276,\n",
    "            \"S\" : 87.03203,\n",
    "            \"T\" : 101.04768,\n",
    "            \"W\" : 186.07931,\n",
    "            \"Y\" : 163.06333,\n",
    "            \"V\" : 99.06841\n",
    "            }\n",
    "    \n",
    "    mass = 0\n",
    "    \n",
    "    for aa in mass_table:\n",
    "        mass += prot_seq.count(aa) * mass_table[aa]\n",
    "    \n",
    "    \n",
    "    ion_mass = mass + (charge_state * 1.007276)\n",
    "    m_z = ion_mass/charge_state\n",
    "    \n",
    "    return m_z\n",
    "\n",
    "\n",
    "def ok_for_MS(pep_list):\n",
    "    \n",
    "    for pep in pep_list:\n",
    "        \n",
    "        pep['okForMS'] = ''\n",
    "        \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 2) < 2000):\n",
    "            pep['okForMS'] += '2'\n",
    "            \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 3) < 2000):\n",
    "            pep['okForMS'] += ',3'\n",
    "        \n",
    "        if len(pep['okForMS']) > 0:\n",
    "            pep['okForMS'] = pep['okForMS'].lstrip(',')\n",
    "        else:\n",
    "            pep['okForMS'] = None\n",
    "            \n",
    "    return pep_list      \n",
    "\n",
    "def make_prot_dict(seq_dict, tm_dict, phob_dict, sp_dict, predisi_dict, SPC_dict):\n",
    "    \n",
    "    prot_dict = dict()\n",
    "    \n",
    "    for ID in seq_dict:\n",
    "      \n",
    "      seq = seq_dict[ID]\n",
    "    \n",
    "      if tm_dict.get(ID) and phob_dict.get(ID) and sp_dict.get(ID) and predisi_dict.get(ID):\n",
    "        \n",
    "        \n",
    "        pep_list = trypsinize(seq)\n",
    "        \n",
    "        pep_list = ok_for_MS(pep_list)\n",
    "\n",
    "        \n",
    "        glyco_indices_S = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]S', seq ):\n",
    "            glyco_indices_S.append(match.start()+1)\n",
    "        \n",
    "        glyco_indices_T = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]T', seq ):\n",
    "            glyco_indices_T.append(match.start()+1)    \n",
    "            \n",
    "        glyco_indices_C = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]C', seq ):\n",
    "            glyco_indices_C.append(match.start()+1)   \n",
    "        \n",
    "        glyco_indices_V = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]V', seq ):\n",
    "            glyco_indices_V.append(match.start()+1)\n",
    "            \n",
    "        K_indices = list()\n",
    "        C_indices = list()\n",
    "        \n",
    "        for i in range(0, len(seq)):\n",
    "            if seq[i] == 'K':\n",
    "                K_indices.append(i+1)\n",
    "            elif seq[i] == 'C':\n",
    "                C_indices.append(i+1) \n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        prot_dict[ID] = {                             \n",
    "                            'seq_info' :\n",
    "                              { \n",
    "                                  'seq' : seq , \n",
    "                                  'seq_len' : len(seq) \n",
    "                              } , \n",
    "                             \n",
    "                            'topo' :\n",
    "                              {\n",
    "                                 'TMHMM' :\n",
    "                                    { 'inside' : tm_dict[ID]['inside']  ,\n",
    "                                      'outside' : tm_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : tm_dict[ID]['num_TM'] ,\n",
    "                                      'stringOut' : tm_dict[ID]['topo'] \n",
    "                                    } ,\n",
    "                                  \n",
    "                                 'Phobius' :\n",
    "                                    { 'inside' : phob_dict[ID]['inside']  , \n",
    "                                      'outside' : phob_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : phob_dict[ID]['num_TM']  ,\n",
    "                                      'stringOut' : phob_dict[ID]['topo'] \n",
    "                                    }\n",
    "                              } , \n",
    "                           \n",
    "                            'signal' : \n",
    "                              {\n",
    "                                  'Phobius' : {'SP' : phob_dict[ID]['SP'] , 'score' : 0 } ,\n",
    "                                  'SignalP' : {'SP' : sp_dict[ID]['SP'] , 'score' : sp_dict[ID]['score'] } ,\n",
    "                                  'PrediSi' : {'SP' : predisi_dict[ID]['SP'] , 'score' : predisi_dict[ID]['score'] }\n",
    "                              } , \n",
    "                            \n",
    "                            'SPC' : {'score' : int(SPC_dict[ID]['score']) , 'stringOut' : SPC_dict[ID]['stringOut']}, \n",
    "            \n",
    "                            'peptides' : pep_list, \n",
    "\n",
    "            \n",
    "                            'motif_sites' : \n",
    "                               { \n",
    "                                 'NXS' :\n",
    "                                   { 'all' : glyco_indices_S , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXT' :\n",
    "                                   { 'all' : glyco_indices_T , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'NXC' :\n",
    "                                   { 'all' : glyco_indices_C , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXV' :\n",
    "                                   { 'all' : glyco_indices_V , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'C' :\n",
    "                                   { 'all' : C_indices , 'extracellular' : dict()  } , \n",
    "                                 \n",
    "                                 'K' :\n",
    "                                   { 'all' : K_indices , 'extracellular' : dict()  } , \n",
    "                                    \n",
    "                               }                          \n",
    "                                           \n",
    "                        }\n",
    "\n",
    "    return prot_dict\n",
    "\n",
    "def EC_analysis(prot):\n",
    "    \n",
    "    motif_list = ['NXS','NXT','NXC','NXV','C','K'] \n",
    "        \n",
    "    for pep in prot['peptides']:\n",
    "\n",
    "        for motif in motif_list:\n",
    "        \n",
    "            pep[motif] = dict()\n",
    "            pep[motif]['extracellular'] = dict()\n",
    "        \n",
    "        for pred in ['TMHMM','Phobius']: \n",
    "            \n",
    "            outside_indices = prot['topo'][pred]['outside']\n",
    "\n",
    "            for motif in motif_list:\n",
    "\n",
    "                motif_indices = prot['motif_sites'][motif]['all']\n",
    "                out_motif = list( set(motif_indices) & set(outside_indices) )\n",
    "\n",
    "                prot['motif_sites'][motif]['extracellular'][pred] = out_motif\n",
    "\n",
    "                all = set(motif_indices) & set(pep['indices'])\n",
    "\n",
    "                if all:    \n",
    "                    pep[motif]['all'] = {'num' : len(all), 'indices' : list(all) }\n",
    "\n",
    "                    out = set(out_motif) & set(pep['indices'])\n",
    "\n",
    "                    if out:\n",
    "                        pep[motif]['extracellular'][pred] = {'num' : len(out), 'indices' : list(out)}\n",
    "                    else:\n",
    "                        pep[motif]['extracellular'][pred] = {'num' : '0', 'indices' : 'n/a' }\n",
    "\n",
    "                else: \n",
    "                    pep[motif]['all'] = {'num' : '0', 'indices' : 'n/a' }\n",
    "                    pep[motif]['extracellular'][pred] = {'num' : '0', 'indices' : 'n/a' }\n",
    "                            \n",
    "\n",
    "    return prot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequnces: 20416\n",
      "SignalP: 20413\n",
      "TMHMM: 20412\n",
      "Phobius: 20412\n",
      "SPC: 20424\n",
      "Predisi: 20416\n"
     ]
    }
   ],
   "source": [
    "file_name = 'can_uniprot-proteome_UP000005640+reviewed_yes.fasta'\n",
    "path = 'io_files/'\n",
    "\n",
    "seqs_dict = fasta_parser(path + file_name)\n",
    "print('sequnces:',len(seqs_dict))\n",
    "\n",
    "sp = parse_signalP(path + 'signalp_out.tsv')\n",
    "print('SignalP:',len(sp))\n",
    "\n",
    "TMHMM_dict  = parse_TMHMM(path + 'TMHMM_out_clean.tsv')\n",
    "print('TMHMM:',len(TMHMM_dict))\n",
    "\n",
    "Phobius_dict = parse_Phobius(path + 'Phobius_out_clean.tsv')\n",
    "print('Phobius:',len(Phobius_dict))\n",
    "\n",
    "SPC_dict = parse_SPC(seqs_dict, path + 'SPC_by_Source.csv')\n",
    "print('SPC:',len(SPC_dict))\n",
    "\n",
    "predisi_dict = parse_Predisi(path + 'predisi.txt')\n",
    "print('Predisi:',len(predisi_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_dict = make_prot_dict(seqs_dict, TMHMM_dict, Phobius_dict, sp, predisi_dict, SPC_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq_info': {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRRGGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLRTQEETLLKLRLASLSQLRRLNSSEAQAPS', 'seq_len': 164}, 'topo': {'TMHMM': {'inside': [123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'outside': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'num_TM': 1, 'stringOut': 'o100-122i'}, 'Phobius': {'inside': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97], 'outside': [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'num_TM': 1, 'stringOut': 'i98-119o'}}, 'signal': {'Phobius': {'SP': '0', 'score': 0}, 'SignalP': {'SP': 0, 'score': '0.001041'}, 'PrediSi': {'SP': 'N', 'score': '0.0027'}}, 'SPC': {'score': 1, 'stringOut': 'dC'}, 'peptides': [{'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'missed_cleavages': 0, 'okForMS': '2,3'}, {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 2, 'okForMS': None}, {'seq': 'SR', 'indices': [31, 32], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'SRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 1, 'okForMS': None}, {'seq': 'SRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRR', 'indices': [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], 'missed_cleavages': 2, 'okForMS': None}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], 'missed_cleavages': 1, 'okForMS': None}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRRGGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 2, 'okForMS': None}, {'seq': 'R', 'indices': [95], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'RGGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'RGGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLR', 'indices': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 2, 'okForMS': '3'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 0, 'okForMS': '2,3'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLR', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 1, 'okForMS': '3'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLRTQEETLLK', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 2, 'okForMS': '3'}, {'seq': 'ILAHTLR', 'indices': [129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 0, 'okForMS': '2,3'}, {'seq': 'ILAHTLRTQEETLLK', 'indices': [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'ILAHTLRTQEETLLKLR', 'indices': [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'missed_cleavages': 2, 'okForMS': '2,3'}, {'seq': 'TQEETLLK', 'indices': [136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 0, 'okForMS': '2,3'}, {'seq': 'TQEETLLKLR', 'indices': [136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'TQEETLLKLRLASLSQLR', 'indices': [136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 2, 'okForMS': '2,3'}, {'seq': 'LR', 'indices': [144, 145], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'LRLASLSQLR', 'indices': [144, 145, 146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'LRLASLSQLRR', 'indices': [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154], 'missed_cleavages': 2, 'okForMS': '2,3'}, {'seq': 'LASLSQLR', 'indices': [146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 0, 'okForMS': '2,3'}, {'seq': 'LASLSQLRR', 'indices': [146, 147, 148, 149, 150, 151, 152, 153, 154], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'LASLSQLRRLNSSEAQAPS', 'indices': [146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 2, 'okForMS': '2,3'}, {'seq': 'R', 'indices': [154], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'RLNSSEAQAPS', 'indices': [154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 1, 'okForMS': '2,3'}, {'seq': 'LNSSEAQAPS', 'indices': [155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 0, 'okForMS': '2,3'}], 'motif_sites': {'NXS': {'all': [156], 'extracellular': {}}, 'NXT': {'all': [], 'extracellular': {}}, 'NXC': {'all': [], 'extracellular': {}}, 'NXV': {'all': [38], 'extracellular': {}}, 'C': {'all': [8, 44, 84, 89, 109], 'extracellular': {}}, 'K': {'all': [143], 'extracellular': {}}}}\n"
     ]
    }
   ],
   "source": [
    "print(str(prot_dict['Q8N112']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pepOut.tsv', 'w') as fo:\n",
    "    \n",
    "    header = ''\n",
    "    fo.write(header + '\\n')\n",
    "    \n",
    "    topo_list = ['all', 'Phobius', 'TMHMM']\n",
    "    motif_list = ['NXS','NXT','NXC','NXV','C','K'] \n",
    "        \n",
    "    for ID in prot_dict:\n",
    "\n",
    "        prot_dict[ID] = EC_analysis(prot_dict[ID])\n",
    "\n",
    "        peps = prot_dict[ID]['peptides']\n",
    "\n",
    "        for pep in peps:\n",
    "\n",
    "            out = list()\n",
    "            pep_range = str(pep['indices'][0]) + '-' + str(pep['indices'][-1])\n",
    "            uniq = ID + '_' + pep_range\n",
    "\n",
    "            out.append(uniq)\n",
    "            out.append(pep['seq'])\n",
    "            out.append(ID)\n",
    "            out.append(pep_range)\n",
    "            out.append(str(pep['missed_cleavages']))\n",
    "\n",
    "            if pep['okForMS']:\n",
    "                out.append(pep['okForMS'])\n",
    "            else:\n",
    "                out.append('None')\n",
    "\n",
    "            for topo in topo_list:\n",
    "\n",
    "                for motif in motif_list:\n",
    "                    if pep[motif]: \n",
    "                        if pep[motif].get(topo):\n",
    "                            out.append(str(pep[motif][topo]['num']))\n",
    "                        elif pep[motif]['extracellular'].get(topo):\n",
    "                            out.append(str(pep[motif]['extracellular'][topo]['num']))\n",
    "                    else:\n",
    "                        out.append('0')\n",
    "\n",
    "                for motif in motif_list:\n",
    "                    if pep[motif]:\n",
    "                        if pep[motif].get(topo):\n",
    "                            out.append(str(pep[motif][topo]['indices']))\n",
    "                        elif pep[motif]['extracellular'].get(topo):\n",
    "                            out.append(str(pep[motif]['extracellular'][topo]['indices']))\n",
    "                    else:\n",
    "                        out.append('0')\n",
    "\n",
    "            fo.write('\\t'.join(out)+'\\n')\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('protOut.tsv', 'w') as fo:\n",
    "    \n",
    "    header = ''\n",
    "    fo.write(header + '\\n')\n",
    "    \n",
    "    motif_list = ['NXS','NXT','NXC','NXV','C','K']\n",
    "    \n",
    "    for ID in prot_dict:\n",
    "        \n",
    "        out = list()     \n",
    "        prot = prot_dict[ID]\n",
    "        \n",
    "        out.append(ID)\n",
    "        \n",
    "        si = prot['seq_info']\n",
    "        out.append(si['seq'])\n",
    "        out.append(str(si['seq_len']))\n",
    "\n",
    "        for meth in ['Phobius', 'TMHMM']:\n",
    "            topo = prot['topo'][meth]\n",
    "                   \n",
    "            out.append(topo['stringOut'])\n",
    "            out.append(str(topo['num_TM']))\n",
    "            out.append(str(len(topo['inside'])))\n",
    "            out.append(str(len(topo['outside'])))\n",
    "                   \n",
    "        for meth in ['Phobius', 'SignalP', 'PrediSi']:\n",
    "            sig = prot['signal'][meth]\n",
    "            sppc = 0\n",
    "                   \n",
    "            if sig['SP'] == 'Y':\n",
    "                out.append('Y')\n",
    "                sppc += 1\n",
    "            else:\n",
    "                out.append('N')\n",
    "                   \n",
    "            if meth == 'Phobius':\n",
    "                out.append('n/a')\n",
    "            else:\n",
    "                out.append(str(sig['score']))\n",
    "        out.append(str(sppc))       \n",
    "        \n",
    "        spc = prot['SPC']\n",
    "        out.append(str(spc['score']))\n",
    "        out.append(spc['stringOut'])      \n",
    "        \n",
    "        prot = EC_analysis(prot)\n",
    "        \n",
    "        ms = prot['motif_sites'] \n",
    "        for loc in ['all', 'Phobius', 'TMHMM']:\n",
    "            for motif in motif_list:\n",
    "                if loc == 'all':\n",
    "                    sites = ms[motif][loc]\n",
    "                else:\n",
    "                    sites = ms[motif]['extracellular'][loc]\n",
    "                \n",
    "                ns = len(sites)\n",
    "                out.append(str(ns))\n",
    "                if ns:\n",
    "                    out.append(str(sites))\n",
    "                else:\n",
    "                    out.append('n/a')\n",
    "                \n",
    "        peps = prot['peptides']\n",
    "        \n",
    "        pep0 = 0\n",
    "        pep1 = 0\n",
    "        pep2 = 0\n",
    "        \n",
    "        ok0 = 0\n",
    "        ok1 = 0\n",
    "        ok2 = 0\n",
    "        \n",
    "        counts = {'all' : dict(), 'Phobius': dict(), 'TMHMM' :dict()}\n",
    "        for key in counts:\n",
    "            counts[key] = [dict(), dict(), dict()]\n",
    "\n",
    "            for i in counts[key]:\n",
    "                for motif in motif_list:\n",
    "                    i[motif] = 0\n",
    "            \n",
    "        for pep in peps:\n",
    "            mc = pep['missed_cleavages']\n",
    "            ok = pep['okForMS']\n",
    "            \n",
    "            if mc == 0:\n",
    "                pep0 += 1\n",
    "                pep1 += 1\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok0 += 1\n",
    "                    ok1 += 1\n",
    "                    ok2 += 1\n",
    "            \n",
    "            elif mc == 1: \n",
    "                pep1 += 1\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok1 += 1\n",
    "                    ok2 += 1\n",
    "                                   \n",
    "            else:\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok2 += 1\n",
    "                    \n",
    "            if ok:\n",
    "                for motif in motif_list:\n",
    "                    for loc in ['all', 'Phobius', 'TMHMM']: \n",
    "                        if loc == 'all':\n",
    "                            if int(pep[motif][loc]['num']) > 0:\n",
    "                                counts[loc][mc][motif] += 1\n",
    "                        else:\n",
    "                            if int(pep[motif]['extracellular'][loc]['num']) > 0:\n",
    "                                counts[loc][mc][motif] += 1\n",
    "                                                        \n",
    "        for var in [pep0, pep1, pep2, ok0, ok1, ok2]:\n",
    "            out.append(str(var))\n",
    "            \n",
    "        for loc in ['all', 'Phobius', 'TMHMM']:\n",
    "            \n",
    "            for i in [0,1,2]:\n",
    "                \n",
    "                for motif in motif_list:\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        count = counts[loc][i][motif]\n",
    "                        \n",
    "                    elif i == 1:\n",
    "                        count = counts[loc][i][motif] + counts[loc][0][motif]\n",
    "                                                \n",
    "                    else:\n",
    "                        count = counts[loc][i][motif] + counts[loc][1][motif] + counts[loc][0][motif]\n",
    "                        \n",
    "                    \n",
    "                    out.append(str(count))\n",
    "    \n",
    "                        \n",
    "        \n",
    "        fo.write('\\t'.join(out)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': [{'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}], 'Phobius': [{'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}], 'TMHMM': [{'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}, {'NXS': 0, 'NXT': 0, 'NXC': 0, 'NXV': 0, 'C': 0, 'K': 0}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counts = {'all' : dict(), 'Phobius': dict(), 'TMHMM' :dict()}\n",
    "for key in counts:\n",
    "    counts[key] = [dict(), dict(), dict()]\n",
    "    \n",
    "    for i in counts[key]:\n",
    "        for motif in motif_list:\n",
    "            i[motif] = 0\n",
    "            \n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "        pep0 = 0\n",
    "        pep1 = 0\n",
    "        pep2 = 0\n",
    "        \n",
    "        ok0 = 0\n",
    "        ok1 = 0\n",
    "        ok2 = 0\n",
    "        \n",
    "        counts = {'all' : dict(), 'Phobius': dict(), 'TMHMM' :dict()}\n",
    "        for key in counts:\n",
    "            counts[key] = [dict(), dict(), dict()]\n",
    "\n",
    "            for i in counts[key]:\n",
    "                for motif in motif_list:\n",
    "                    i[motif] = 0\n",
    "    \n",
    "        for pep in peps:\n",
    "            mc = pep['missed_cleavages']\n",
    "            ok = pep['okForMS']\n",
    "            \n",
    "            if mc == 0:\n",
    "                pep0 += 1\n",
    "                pep1 += 1\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok0 += 1\n",
    "                    ok1 += 1\n",
    "                    ok2 += 1\n",
    "            \n",
    "            elif mc == 1: \n",
    "                pep1 += 1\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok1 += 1\n",
    "                    ok2 += 1\n",
    "                                   \n",
    "            else:\n",
    "                pep2 += 1\n",
    "                if ok:\n",
    "                    ok2 += 1\n",
    "                    \n",
    "            if ok:\n",
    "                for motif in motif_list:\n",
    "                    for loc in ['all', 'Phobius', 'TMHMM']: \n",
    "                        if loc == 'all':\n",
    "                            if int(pep[motif][loc]['num']) > 0:\n",
    "                                counts[loc][mc][motif] += 1\n",
    "                        else:\n",
    "                            if int(pep[motif]['extracellular'][loc]['num']) > 0:\n",
    "                                counts[loc][mc][motif] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
