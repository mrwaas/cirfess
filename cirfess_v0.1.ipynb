{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Motif Coverage Predictor\n",
    "\n",
    "This program is designed to return the number of peptides for a protein (or set of proteins) which contain a given sequence motif (N-glyco motif) and are suitable for mass spectrometry. \n",
    "\n",
    "The program considers protein-level information from prediction tools such as `TMHMM`, `Phobius`, and `SignalP` in order to better interpret the context of possible sequence motifs. \n",
    "\n",
    "## I/O and Usage\n",
    "\n",
    "### Usage\n",
    "\n",
    "### Input\n",
    "\n",
    "\n",
    "### Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the definitions of functions to parse the various input file types, including:\n",
    "    # TMHMM short format output\n",
    "    # Phobius short format output\n",
    "    # Signal P short format output\n",
    "    # Fasta file\n",
    "\n",
    "# \n",
    "\n",
    "# Dependencies\n",
    "    # regular expressions (re)\n",
    "\n",
    "\n",
    "\n",
    "def parse_TMHMM(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        tm_dict = dict()\n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID = line.split('\\t')[0]\n",
    "            length = int(line.split('\\t')[1].split('=')[1])\n",
    "            topo_str = line.split('\\t')[5].split('=')[1]\n",
    "            num_TM = int(line.split('\\t')[4].split('=')[1])\n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : None }\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : topo_str } \n",
    "                \n",
    "        \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                   i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                   o1.append(1)\n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "                    \n",
    "    return tm_dict\n",
    "\n",
    "def parse_Phobius(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "        tm_dict = dict()\n",
    "        \n",
    "        with open('io_files/human_plus_leftovers.tab' , 'r') as fo2:\n",
    "            length_dict = dict()\n",
    "            header = True\n",
    "            for line in fo2:\n",
    "                if header:\n",
    "                    header = False\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    length = line.split('\\t')[6]\n",
    "                    length_dict[ID] = int(length)\n",
    "                            \n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID, num_TM, SP, topo_str = line.split()\n",
    "            ID = ID.split('|')[1]\n",
    "            \n",
    "            if bool(length_dict.get(ID)):\n",
    "                length = length_dict[ID]\n",
    "            else:\n",
    "                continue   \n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : None , \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : topo_str ,  \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "                \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                    i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                    o1.append(1)\n",
    "                else:\n",
    "                    match = re.search(r'\\S+/(\\S+?)([io])(\\S*)', topo)\n",
    "                    if match:\n",
    "                        topo = match.group(2) + match.group(3)\n",
    "                    else:\n",
    "                        print(topo)\n",
    "                    if topo.startswith('i'):\n",
    "                        i1.append(int(match.group(1)))\n",
    "                    elif topo.startswith('o'):\n",
    "                        o1.append(int(match.group(1)))\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def parse_signalP(file_name):\n",
    "    \n",
    "        with open(file_name, 'r') as fo:\n",
    "            \n",
    "            SP_dict = dict()\n",
    "            \n",
    "            for line in fo:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    SP = line.split()[1]\n",
    "                    \n",
    "                    if SP.startswith('SP'):\n",
    "                        SP = 'Y'\n",
    "                    else:\n",
    "                        SP = 0\n",
    "            \n",
    "                    SP_dict[ID] = { 'SP': SP } \n",
    "        \n",
    "        return SP_dict\n",
    "    \n",
    "def parse_Predisi(file_name):\n",
    "    \n",
    "    with open (file_name, 'r') as fo:\n",
    "        \n",
    "        predisi_dict = dict()\n",
    "        header = True\n",
    "        \n",
    "        for line in fo:\n",
    "            if header:\n",
    "                header = False\n",
    "            else:\n",
    "                line = line.rstrip()\n",
    "                ID = line.split('\\t')[0].split('|')[1]\n",
    "                SP = line.split('\\t')[-2]\n",
    "                \n",
    "                predisi_dict[ID] = {'SP' : SP}\n",
    "                \n",
    "                \n",
    "    return predisi_dict\n",
    "    \n",
    "def parse_SPC(seq_dict, file_name):\n",
    "    \n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "            SPC_dict = dict()\n",
    "            header = True\n",
    "\n",
    "            for line in fo:\n",
    "                    if header:\n",
    "                        header = False\n",
    "                    else:\n",
    "                        line = line.rstrip()\n",
    "                        ID, SPC = line.split(',')[0], line.split(',')[1]\n",
    "\n",
    "                        SPC_dict[ID] = int(SPC)\n",
    "            \n",
    "            for ID in seq_dict:\n",
    "                \n",
    "                if SPC_dict.get(ID) == None:\n",
    "                    SPC_dict[ID] = 0\n",
    "            \n",
    "    return SPC_dict\n",
    "        \n",
    "        \n",
    "            \n",
    "def fasta_parser(fasta_filename):   \n",
    "    \n",
    "\n",
    "    fasta_fileobj = open(fasta_filename, 'r')\t## create a file obj from the specified file\n",
    "\n",
    "    sequence_name = ''\t\t\t\t## initialize strings to populate from file object info\n",
    "    sequence_desc = ''\n",
    "    sequence_string = ''\n",
    "    sequence_dict = {}\n",
    "\n",
    "    for line in fasta_fileobj:  \t\t\t## iterate through file object with for loop\n",
    "        line = line.rstrip()\t\t\t## strip white space on the right side (like a new line character!) \n",
    "\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(sequence_string) > 0:\n",
    "                sequence_dict[sequence_name] = sequence_string\t\n",
    "                sequence_string = ''  \t\t## reset for the new sequence\n",
    "            \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  ## split on only first space\n",
    "            sequence_name = sequence_info[0].split('|')[1]\n",
    "\t\n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "            else:\t\t\t\t\t## sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "\t\t\n",
    "           \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  \t## split on only first space\n",
    "           \n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "           \n",
    "            else:\n",
    "            # sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "             \n",
    "        else:\n",
    "            sequence_string += line  # incrementally elongate seq\n",
    "\n",
    "# When we reach the end of the FASTA file, we drop out of the\n",
    "# 'for' loop. However, we still have the last sequence record\n",
    "# stored in memory, which we haven't processed yet, because we\n",
    "# haven't observed a '>' symbol, so we must copy and paste any\n",
    "# code that we used to process sequences above to the code block\n",
    "# below. Check if sequence_string has a non-zero length to\n",
    "# determine whether to execute the sequence processing code:\n",
    "\n",
    "    if len(sequence_string) > 0:\n",
    "        sequence_dict[sequence_name] = sequence_string\n",
    "        \n",
    "    return sequence_dict\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contatins the definition of functions required \n",
    "\n",
    "import re\n",
    "\n",
    "## 2 missed cleavages are used for trypsinize, where the number of missed cleavages are recorded\n",
    "\n",
    "def trypsinize(prot_seq):\n",
    "    peptides= []\n",
    "    cut_sites=[0]\n",
    "    indices = []\n",
    "    pep = ''\n",
    "\n",
    "    for i in range(0,len(prot_seq)-1):\n",
    "        if prot_seq[i] == 'K' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        elif prot_seq[i] == 'R' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        \n",
    "    if cut_sites[-1]!=len(prot_seq):\n",
    "            cut_sites.append(len(prot_seq))\n",
    "            \n",
    "    if len(cut_sites)>2:\n",
    "            \n",
    "        for j in range(0,len(cut_sites)-3):\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+2]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+2]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+3]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+3]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 2})\n",
    "            indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-2]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-2]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-2]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-2],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "                    \n",
    "    else: #there is no trypsin site in the protein sequence\n",
    "        peptides.append({'seq' : prot_seq, 'indices' : range(1,len(prot_seq)+1), 'missed_cleavages' : 0})\n",
    "        \n",
    "    return peptides\n",
    "\n",
    "def ion_mim(prot_seq, charge_state):\n",
    "    \n",
    "    mass_table = {\n",
    "            \"A\" : 71.03711,\n",
    "            \"R\" : 156.10111,\n",
    "            \"N\" : 114.04293,\n",
    "            \"D\" : 115.02694,\n",
    "            \"C\" : 103.00919 + 57.02146,\n",
    "            \"E\" : 129.04259,\n",
    "            \"Q\" : 128.05858,\n",
    "            \"G\" : 57.02146,\n",
    "            \"H\" : 137.05891,\n",
    "            \"I\" : 113.08406,\n",
    "            \"L\" : 113.08406,\n",
    "            \"K\" : 128.09496,\n",
    "            \"M\" : 131.04049,\n",
    "            \"F\" : 147.06841,\n",
    "            \"P\" : 97.05276,\n",
    "            \"S\" : 87.03203,\n",
    "            \"T\" : 101.04768,\n",
    "            \"W\" : 186.07931,\n",
    "            \"Y\" : 163.06333,\n",
    "            \"V\" : 99.06841\n",
    "            }\n",
    "    \n",
    "    mass = 0\n",
    "    \n",
    "    for aa in mass_table:\n",
    "        mass += prot_seq.count(aa) * mass_table[aa]\n",
    "    \n",
    "    \n",
    "    ion_mass = mass + (charge_state * 1.007276)\n",
    "    m_z = ion_mass/charge_state\n",
    "    \n",
    "    return m_z\n",
    "\n",
    "\n",
    "def ok_for_MS(pep_list):\n",
    "    \n",
    "    for pep in pep_list:\n",
    "        \n",
    "        pep['okForMS'] = ''\n",
    "        \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 2) < 2000):\n",
    "            pep['okForMS'] += '2'\n",
    "            \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 3) < 2000):\n",
    "            pep['okForMS'] += ',3'\n",
    "        \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 4) < 2000):    \n",
    "            pep['okForMS'] += ',4'\n",
    "        \n",
    "        else:\n",
    "            pep['okForMS'] = None\n",
    "            \n",
    "    return pep_list      \n",
    "\n",
    "def make_prot_dict(seq_dict, tm_dict, phob_dict, sp_dict, predisi_dict, SPC_dict):\n",
    "    \n",
    "    prot_dict = dict()\n",
    "    \n",
    "    for ID in seq_dict:\n",
    "      \n",
    "      seq = seq_dict[ID]\n",
    "    \n",
    "      if tm_dict.get(ID) and phob_dict.get(ID) and sp_dict.get(ID) and predisi_dict.get(ID):\n",
    "        \n",
    "        \n",
    "        pep_list = trypsinize(seq)\n",
    "        \n",
    "        pep_list = ok_for_MS(pep_list)\n",
    "\n",
    "        \n",
    "        glyco_indices_S = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]S', seq ):\n",
    "            glyco_indices_S.append(match.start()+1)\n",
    "        \n",
    "        glyco_indices_T = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]T', seq ):\n",
    "            glyco_indices_T.append(match.start()+1)    \n",
    "            \n",
    "        glyco_indices_C = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]C', seq ):\n",
    "            glyco_indices_C.append(match.start()+1)   \n",
    "        \n",
    "        glyco_indices_V = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]V', seq ):\n",
    "            glyco_indices_V.append(match.start()+1)\n",
    "            \n",
    "        K_indices = list()\n",
    "        C_indices = list()\n",
    "        \n",
    "        for i in range(0, len(seq)):\n",
    "            if seq[i] == 'K':\n",
    "                K_indices.append(i+1)\n",
    "            elif seq[i] == 'C':\n",
    "                C_indices.append(i+1) \n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        prot_dict[ID] = {                             \n",
    "                            'seq_info' :\n",
    "                              { \n",
    "                                  'seq' : seq , \n",
    "                                  'seq_len' : len(seq) \n",
    "                              } , \n",
    "                             \n",
    "                            'topo' :\n",
    "                              {\n",
    "                                 'TMHMM' :\n",
    "                                    { 'inside' : tm_dict[ID]['inside']  ,\n",
    "                                      'outside' : tm_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : tm_dict[ID]['num_TM'] } ,\n",
    "                                 'Phobius' :\n",
    "                                    { 'inside' : phob_dict[ID]['inside']  , \n",
    "                                      'outside' : phob_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : phob_dict[ID]['num_TM'] }  \n",
    "                              } , \n",
    "                           \n",
    "                            'signal' : \n",
    "                              {\n",
    "                                  'Phobius' : phob_dict[ID]['SP'] ,\n",
    "                                  'SignalP' : sp_dict[ID]['SP'] , \n",
    "                                  'PrediSi' : predisi_dict[ID]['SP']  \n",
    "                              } , \n",
    "                            \n",
    "                            'SPC' : int(SPC_dict[ID]), \n",
    "            \n",
    "                            'peptides' : pep_list, \n",
    "\n",
    "            \n",
    "                            'motif_sites' : \n",
    "                               { \n",
    "                                 'NXS' :\n",
    "                                   { 'all' : glyco_indices_S , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXT' :\n",
    "                                   { 'all' : glyco_indices_T , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'NXC' :\n",
    "                                   { 'all' : glyco_indices_C , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXV' :\n",
    "                                   { 'all' : glyco_indices_V , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'C' :\n",
    "                                   { 'all' : C_indices , 'extracellular' : dict()  } , \n",
    "                                 \n",
    "                                 'K' :\n",
    "                                   { 'all' : K_indices , 'extracellular' : dict()  } , \n",
    "                                    \n",
    "                               }                          \n",
    "                                           \n",
    "                        }\n",
    "    print('step1_done')\n",
    "\n",
    "    \n",
    "    return prot_dict\n",
    "\n",
    "def EC_analysis(prot_dict, pred_method):\n",
    "    count = 0\n",
    "    \n",
    "    motif_list = ['NXS','NXT','NXC','NXV','C','K'] \n",
    "        \n",
    "    for pred in pred_method: \n",
    "    \n",
    "        for ID, value in prot_dict.items():\n",
    "            count += 1\n",
    "\n",
    "            if count%200 == 0:\n",
    "                print('count is at',str(count))\n",
    "            \n",
    "            outside_indices = value['topo'][pred]['outside']\n",
    "            \n",
    "            if len(outside_indices) == 0:\n",
    "                for motif in motif_list:\n",
    "                    value['motif_sites'][motif]['extracellular'][pred] = []\n",
    "            \n",
    "            else: \n",
    "                for motif in motif_list:\n",
    "                    \n",
    "                    motif_indices = value['motif_sites'][motif]['all']\n",
    "                    out_motif = list( set(motif_indices) & set(outside_indices) )\n",
    "\n",
    "                    value['motif_sites'][motif]['extracellular'][pred] = out_motif \n",
    "\n",
    "                    for pep in value['peptides']:\n",
    "\n",
    "                        if pep['okForMS'] == None:\n",
    "                            pep[motif] = None\n",
    "\n",
    "                        else:                 \n",
    "                            pep[motif] = dict()\n",
    "                            pep[motif]['extracellular'] = dict()\n",
    "\n",
    "                            all = set(motif_indices) & set(pep['indices'])\n",
    "                            if all:    \n",
    "                                pep[motif]['all'] = {'num' : len(all), 'indices' : list(all) }\n",
    "\n",
    "                                out = set(out_motif) & set(pep['indices'])\n",
    "\n",
    "                                if out:\n",
    "                                    pep[motif]['extracellular'][pred] = {'num' : len(out), 'indices' : list(out)}\n",
    "\n",
    "    return prot_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequnces: 20416\n",
      "SignalP: 20413\n",
      "TMHMM: 20412\n",
      "Phobius: 20412\n",
      "SPC: 20424\n",
      "Predisi: 20416\n"
     ]
    }
   ],
   "source": [
    "file_name = 'can_uniprot-proteome_UP000005640+reviewed_yes.fasta'\n",
    "path = 'io_files/'\n",
    "\n",
    "seqs_dict = fasta_parser(path + file_name)\n",
    "print('sequnces:',len(seqs_dict))\n",
    "\n",
    "sp = parse_signalP(path + 'signalp_out.tsv')\n",
    "print('SignalP:',len(sp))\n",
    "\n",
    "TMHMM_dict  = parse_TMHMM(path + 'TMHMM_out_clean.tsv')\n",
    "print('TMHMM:',len(TMHMM_dict))\n",
    "\n",
    "Phobius_dict = parse_Phobius(path + 'Phobius_out_clean.tsv')\n",
    "print('Phobius:',len(Phobius_dict))\n",
    "\n",
    "SPC_dict = parse_SPC(seqs_dict, path + 'SPC_by_Source.csv')\n",
    "print('SPC:',len(SPC_dict))\n",
    "\n",
    "predisi_dict = parse_Predisi(path + 'predisi.txt')\n",
    "print('Predisi:',len(predisi_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1_done\n"
     ]
    }
   ],
   "source": [
    "prot_dict = make_prot_dict(seqs_dict, TMHMM_dict, Phobius_dict, sp, predisi_dict, SPC_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prot dict: 20405\n"
     ]
    }
   ],
   "source": [
    "print('Prot dict:', len(prot_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3629130\n",
      "850683\n"
     ]
    }
   ],
   "source": [
    "pep_count = 0\n",
    "not_ok = 0\n",
    "\n",
    "for prot in prot_dict:\n",
    "    pep_count += len(prot_dict[prot]['peptides'])\n",
    "\n",
    "    for pep in prot_dict[prot]['peptides']:\n",
    "        if pep['okForMS'] == None:\n",
    "            not_ok += 1\n",
    "            \n",
    "print(pep_count)\n",
    "print(not_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is at 200\n",
      "count is at 400\n",
      "count is at 600\n",
      "count is at 800\n",
      "count is at 1000\n",
      "count is at 1200\n",
      "count is at 1400\n",
      "count is at 1600\n",
      "count is at 1800\n",
      "count is at 2000\n",
      "count is at 2200\n",
      "count is at 2400\n",
      "count is at 2600\n",
      "count is at 2800\n",
      "count is at 3000\n",
      "count is at 3200\n",
      "count is at 3400\n",
      "count is at 3600\n",
      "count is at 3800\n",
      "count is at 4000\n",
      "count is at 4200\n",
      "count is at 4400\n",
      "count is at 4600\n",
      "count is at 4800\n",
      "count is at 5000\n",
      "count is at 5200\n",
      "count is at 5400\n",
      "count is at 5600\n",
      "count is at 5800\n",
      "count is at 6000\n",
      "count is at 6200\n",
      "count is at 6400\n",
      "count is at 6600\n",
      "count is at 6800\n",
      "count is at 7000\n",
      "count is at 7200\n",
      "count is at 7400\n",
      "count is at 7600\n",
      "count is at 7800\n",
      "count is at 8000\n",
      "count is at 8200\n",
      "count is at 8400\n",
      "count is at 8600\n",
      "count is at 8800\n",
      "count is at 9000\n",
      "count is at 9200\n",
      "count is at 9400\n",
      "count is at 9600\n",
      "count is at 9800\n",
      "count is at 10000\n",
      "count is at 10200\n",
      "count is at 10400\n",
      "count is at 10600\n",
      "count is at 10800\n",
      "count is at 11000\n",
      "count is at 11200\n",
      "count is at 11400\n",
      "count is at 11600\n",
      "count is at 11800\n",
      "count is at 12000\n",
      "count is at 12200\n",
      "count is at 12400\n",
      "count is at 12600\n",
      "count is at 12800\n",
      "count is at 13000\n",
      "count is at 13200\n",
      "count is at 13400\n",
      "count is at 13600\n",
      "count is at 13800\n",
      "count is at 14000\n",
      "count is at 14200\n",
      "count is at 14400\n",
      "count is at 14600\n",
      "count is at 14800\n",
      "count is at 15000\n",
      "count is at 15200\n",
      "count is at 15400\n",
      "count is at 15600\n",
      "count is at 15800\n",
      "count is at 16000\n",
      "count is at 16200\n",
      "count is at 16400\n",
      "count is at 16600\n",
      "count is at 16800\n",
      "count is at 17000\n",
      "count is at 17200\n",
      "count is at 17400\n",
      "count is at 17600\n",
      "count is at 17800\n",
      "count is at 18000\n",
      "count is at 18200\n",
      "count is at 18400\n",
      "count is at 18600\n",
      "count is at 18800\n",
      "count is at 19000\n",
      "count is at 19200\n",
      "count is at 19400\n",
      "count is at 19600\n",
      "count is at 19800\n",
      "count is at 20000\n",
      "count is at 20200\n",
      "count is at 20400\n",
      "count is at 20600\n",
      "count is at 20800\n",
      "count is at 21000\n",
      "count is at 21200\n",
      "count is at 21400\n",
      "count is at 21600\n",
      "count is at 21800\n",
      "count is at 22000\n",
      "count is at 22200\n",
      "count is at 22400\n",
      "count is at 22600\n",
      "count is at 22800\n",
      "count is at 23000\n",
      "count is at 23200\n",
      "count is at 23400\n",
      "count is at 23600\n",
      "count is at 23800\n",
      "count is at 24000\n",
      "count is at 24200\n",
      "count is at 24400\n",
      "count is at 24600\n",
      "count is at 24800\n",
      "count is at 25000\n",
      "count is at 25200\n",
      "count is at 25400\n",
      "count is at 25600\n",
      "count is at 25800\n",
      "count is at 26000\n",
      "count is at 26200\n",
      "count is at 26400\n",
      "count is at 26600\n",
      "count is at 26800\n",
      "count is at 27000\n",
      "count is at 27200\n",
      "count is at 27400\n",
      "count is at 27600\n",
      "count is at 27800\n",
      "count is at 28000\n",
      "count is at 28200\n",
      "count is at 28400\n",
      "count is at 28600\n",
      "count is at 28800\n",
      "count is at 29000\n",
      "count is at 29200\n",
      "count is at 29400\n",
      "count is at 29600\n",
      "count is at 29800\n",
      "count is at 30000\n",
      "count is at 30200\n",
      "count is at 30400\n",
      "count is at 30600\n",
      "count is at 30800\n",
      "count is at 31000\n",
      "count is at 31200\n",
      "count is at 31400\n",
      "count is at 31600\n",
      "count is at 31800\n",
      "count is at 32000\n",
      "count is at 32200\n",
      "count is at 32400\n",
      "count is at 32600\n",
      "count is at 32800\n",
      "count is at 33000\n",
      "count is at 33200\n",
      "count is at 33400\n",
      "count is at 33600\n",
      "count is at 33800\n",
      "count is at 34000\n",
      "count is at 34200\n",
      "count is at 34400\n",
      "count is at 34600\n",
      "count is at 34800\n",
      "count is at 35000\n",
      "count is at 35200\n",
      "count is at 35400\n",
      "count is at 35600\n",
      "count is at 35800\n",
      "count is at 36000\n",
      "count is at 36200\n",
      "count is at 36400\n",
      "count is at 36600\n",
      "count is at 36800\n",
      "count is at 37000\n",
      "count is at 37200\n",
      "count is at 37400\n",
      "count is at 37600\n",
      "count is at 37800\n",
      "count is at 38000\n",
      "count is at 38200\n",
      "count is at 38400\n",
      "count is at 38600\n",
      "count is at 38800\n",
      "count is at 39000\n",
      "count is at 39200\n",
      "count is at 39400\n",
      "count is at 39600\n",
      "count is at 39800\n",
      "count is at 40000\n",
      "count is at 40200\n",
      "count is at 40400\n",
      "count is at 40600\n",
      "count is at 40800\n"
     ]
    }
   ],
   "source": [
    "pred_methods = ['TMHMM','Phobius']\n",
    "\n",
    "EC_analysis(prot_dict, pred_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq_info': {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRRGGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLRTQEETLLKLRLASLSQLRRLNSSEAQAPS', 'seq_len': 164}, 'topo': {'TMHMM': {'inside': [123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'outside': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'num_TM': 1}, 'Phobius': {'inside': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97], 'outside': [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'num_TM': 1}}, 'signal': {'Phobius': '0', 'SignalP': 0, 'PrediSi': 'N'}, 'SPC': 1, 'peptides': [{'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'missed_cleavages': 0, 'okForMS': '2,3,4'}, {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'MPSLAPDCPLLAMPEETQEDSVAPMMPSQRSRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 2, 'okForMS': None}, {'seq': 'SR', 'indices': [31, 32], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'SRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 1, 'okForMS': ',4'}, {'seq': 'SRGPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRR', 'indices': [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], 'missed_cleavages': 2, 'okForMS': ',4'}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'missed_cleavages': 0, 'okForMS': ',4'}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], 'missed_cleavages': 1, 'okForMS': ',4'}, {'seq': 'GPLAPNHVHEVCLHQVESISDLHSGAGTLRPYLTEEARPWDELLGVLPPSLCAQAGCSPVYRRGGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 2, 'okForMS': None}, {'seq': 'R', 'indices': [95], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'RGGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'RGGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLR', 'indices': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 2, 'okForMS': ',3,4'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLR', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128], 'missed_cleavages': 0, 'okForMS': '2,3,4'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLR', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 1, 'okForMS': ',3,4'}, {'seq': 'GGFLLLLALLVLTCLVLALLAVYLSVLQSESLRILAHTLRTQEETLLK', 'indices': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 2, 'okForMS': ',3,4'}, {'seq': 'ILAHTLR', 'indices': [129, 130, 131, 132, 133, 134, 135], 'missed_cleavages': 0, 'okForMS': '2,3,4'}, {'seq': 'ILAHTLRTQEETLLK', 'indices': [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'ILAHTLRTQEETLLKLR', 'indices': [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'missed_cleavages': 2, 'okForMS': '2,3,4'}, {'seq': 'TQEETLLK', 'indices': [136, 137, 138, 139, 140, 141, 142, 143], 'missed_cleavages': 0, 'okForMS': '2,3,4'}, {'seq': 'TQEETLLKLR', 'indices': [136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'TQEETLLKLRLASLSQLR', 'indices': [136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 2, 'okForMS': '2,3,4'}, {'seq': 'LR', 'indices': [144, 145], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'LRLASLSQLR', 'indices': [144, 145, 146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'LRLASLSQLRR', 'indices': [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154], 'missed_cleavages': 2, 'okForMS': '2,3,4'}, {'seq': 'LASLSQLR', 'indices': [146, 147, 148, 149, 150, 151, 152, 153], 'missed_cleavages': 0, 'okForMS': '2,3,4'}, {'seq': 'LASLSQLRR', 'indices': [146, 147, 148, 149, 150, 151, 152, 153, 154], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'LASLSQLRRLNSSEAQAPS', 'indices': [146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 2, 'okForMS': '2,3,4'}, {'seq': 'R', 'indices': [154], 'missed_cleavages': 0, 'okForMS': None}, {'seq': 'RLNSSEAQAPS', 'indices': [154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 1, 'okForMS': '2,3,4'}, {'seq': 'LNSSEAQAPS', 'indices': [155, 156, 157, 158, 159, 160, 161, 162, 163, 164], 'missed_cleavages': 0, 'okForMS': '2,3,4'}], 'motif_sites': {'NXS': {'all': [156], 'extracellular': {}}, 'NXT': {'all': [], 'extracellular': {}}, 'NXC': {'all': [], 'extracellular': {}}, 'NXV': {'all': [38], 'extracellular': {}}, 'C': {'all': [8, 44, 84, 89, 109], 'extracellular': {}}, 'K': {'all': [143], 'extracellular': {}}}}\n"
     ]
    }
   ],
   "source": [
    "print(prot_dict['Q8N112'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for proteins of various num TM domains, report the ratios of EC residues to IC residues\n",
    "\n",
    "with open('TManalysis.tab', 'w') as fo:\n",
    "\n",
    "    header = 'Accession'\n",
    "    \n",
    "    for pred in ['TMHMM', 'Phobius', 'consensus', 'inclusive']:\n",
    "        header += '\\t' + pred+' #TM' +'\\t'+ pred+'I' +'\\t'+ pred+'O' +'\\t'+ pred+'Ratio(O/I)'\n",
    "        \n",
    "    fo.write(header+'\\n')\n",
    "    \n",
    "    for ID in prot_dict:\n",
    "        out = ''\n",
    "        out += ID\n",
    "        \n",
    "        prot = prot_dict[ID]\n",
    "                \n",
    "        for pred in ['TMHMM', 'Phobius', 'consensus', 'inclusive']:\n",
    "            num_TM = prot['topo'][pred]['num_TM']\n",
    "            I = len(prot['topo'][pred]['inside'])\n",
    "            O = len(prot['topo'][pred]['outside'])\n",
    "            \n",
    "            if I == 0:\n",
    "                ratio = 'inf'\n",
    "            else:\n",
    "                ratio = str(O/I)\n",
    "            \n",
    "            out += '\\t'+ str(num_TM) +'\\t'+ str(I) +'\\t'+ str(O) +'\\t'+ ratio\n",
    "            \n",
    "        fo.write(out +'\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each glycomotif record the number that are predicted extracellular, have SPC, have SPC\n",
    "\n",
    "counts = { \n",
    "    'no_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } , \n",
    "    'SPC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } ,\n",
    "    'SP_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'EC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SP-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-SP' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'all_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'total' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 }\n",
    "    }\n",
    "\n",
    "for ID in prot_dict :\n",
    "    prot = prot_dict[ID]\n",
    "    \n",
    "    SP = prot['signal']['inclusive']\n",
    "    SPC = \n",
    "    \n",
    "    for site in prot['glyco_sites']:\n",
    "        sites_dict = prot['glyco_sites'][site]\n",
    "        \n",
    "        all = set(sites_dict['all'])\n",
    "        inclusive = set(sites_dict['extracellular']['inclusive'])\n",
    "        num_ic = len(all) - len(inclusive)\n",
    "    \n",
    "        counts['total'][site] += len(all)\n",
    "        \n",
    "        if SP == 'Y' and SPC > 0:\n",
    "            counts['all_info'][site] += len(inclusive)\n",
    "            counts['SPC-SP'][site] += num_ic\n",
    "            \n",
    "        elif SP == 'Y' and SPC == 0:\n",
    "            counts['SP-EC'][site] += len(inclusive)\n",
    "            counts['SP_only'][site] += num_ic\n",
    "            \n",
    "        elif SP != 'Y' and SPC > 0:\n",
    "            counts['SPC-EC'][site] += len(inclusive)\n",
    "            counts['SPC_only'][site] += num_ic\n",
    "            \n",
    "        else:\n",
    "            counts['EC_only'][site] += len(inclusive)\n",
    "            counts['no_info'][site] += num_ic\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## for each accession --> report SPC, signal, and TM --> report number of glycopeptide, number of EC-glycopeptides \n",
    "\n",
    "with open('glycopep.tab', 'w') as fo:\n",
    "\n",
    "    header = 'Accession\\tSPC\\tspInc\\tspCon\\tspTot\\ttmInc\\ttmCon'\n",
    "    for aa in ['S', 'T', 'C', 'V']:\n",
    "        header += '\\t'+ aa+'gp' +'\\t'+ aa+'gpInc' +'\\t'+ aa+'gpCon'\n",
    "    fo.write(header+'\\n')\n",
    "    \n",
    "    for ID in prot_dict:\n",
    "        out = ''\n",
    "                \n",
    "        prot = prot_dict[ID]\n",
    "        SPC = str(prot['SPC'])\n",
    "        spInc = prot['signal']['inclusive'] \n",
    "        spCon = prot['signal']['consensus']\n",
    "        spTot = 0\n",
    "        \n",
    "        if prot['signal']['Phobius']  == 'Y':\n",
    "            spTot += 1\n",
    "        if prot['signal']['PrediSi']  == 'Y':\n",
    "            spTot += 1\n",
    "        if prot['signal']['SignalP']  == 'Y':\n",
    "            spTot += 1            \n",
    "            \n",
    "        tmInc = str(prot['topo']['inclusive']['num_TM'])\n",
    "        tmCon = str(prot['topo']['consensus']['num_TM'])\n",
    "        \n",
    "        out += ID +'\\t'+ SPC +'\\t'+ spInc +'\\t'+ spCon +'\\t'+ str(spTot) +'\\t'+ tmInc +'\\t'+ tmCon \n",
    "        \n",
    "        for aa in ['S', 'T', 'C', 'V']:\n",
    "            gp = len(prot['glycopeptides'][aa]['2']['all'])\n",
    "            ECgpInc =  len(prot['glycopeptides'][aa]['2']['extracellular']['inclusive'])\n",
    "            ECgpCon =  len(prot['glycopeptides'][aa]['2']['extracellular']['consensus'])\n",
    "            \n",
    "            out += '\\t' + str(gp) +'\\t'+ str(ECgpInc) +'\\t'+ str(ECgpCon) \n",
    "            \n",
    "        fo.write(out + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prot_dict_new.out', 'w') as fo:\n",
    "    fo.write(str(prot_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
