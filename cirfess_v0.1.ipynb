{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Motif Coverage Predictor\n",
    "\n",
    "This program is designed to return the number of peptides for a protein (or set of proteins) which contain a given sequence motif (N-glyco motif) and are suitable for mass spectrometry. \n",
    "\n",
    "The program considers protein-level information from prediction tools such as `TMHMM`, `Phobius`, and `SignalP` in order to better interpret the context of possible sequence motifs. \n",
    "\n",
    "## I/O and Usage\n",
    "\n",
    "### Usage\n",
    "\n",
    "### Input\n",
    "\n",
    "\n",
    "### Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the definitions of functions to parse the various input file types, including:\n",
    "    # TMHMM short format output\n",
    "    # Phobius short format output\n",
    "    # Signal P short format output\n",
    "    # Fasta file\n",
    "\n",
    "# \n",
    "\n",
    "# Dependencies\n",
    "    # regular expressions (re)\n",
    "\n",
    "\n",
    "\n",
    "def parse_TMHMM(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        tm_dict = dict()\n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID = line.split('\\t')[0]\n",
    "            length = int(line.split('\\t')[1].split('=')[1])\n",
    "            topo_str = line.split('\\t')[5].split('=')[1]\n",
    "            num_TM = int(line.split('\\t')[4].split('=')[1])\n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : None }\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : topo_str } \n",
    "                \n",
    "        \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                   i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                   o1.append(1)\n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "                    \n",
    "    return tm_dict\n",
    "\n",
    "def parse_Phobius(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "        tm_dict = dict()\n",
    "        \n",
    "        with open('io_files/human_plus_leftovers.tab' , 'r') as fo2:\n",
    "            length_dict = dict()\n",
    "            header = True\n",
    "            for line in fo2:\n",
    "                if header:\n",
    "                    header = False\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    length = line.split('\\t')[6]\n",
    "                    length_dict[ID] = int(length)\n",
    "                            \n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID, num_TM, SP, topo_str = line.split()\n",
    "            ID = ID.split('|')[1]\n",
    "            \n",
    "            if bool(length_dict.get(ID)):\n",
    "                length = length_dict[ID]\n",
    "            else:\n",
    "                continue   \n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : None , \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : topo_str ,  \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "                \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                    i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                    o1.append(1)\n",
    "                else:\n",
    "                    match = re.search(r'\\S+/(\\S+?)([io])(\\S*)', topo)\n",
    "                    if match:\n",
    "                        topo = match.group(2) + match.group(3)\n",
    "                    else:\n",
    "                        print(topo)\n",
    "                    if topo.startswith('i'):\n",
    "                        i1.append(int(match.group(1)))\n",
    "                    elif topo.startswith('o'):\n",
    "                        o1.append(int(match.group(1)))\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def parse_signalP(file_name):\n",
    "    \n",
    "        with open(file_name, 'r') as fo:\n",
    "            \n",
    "            SP_dict = dict()\n",
    "            \n",
    "            for line in fo:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    SP = line.split()[1]\n",
    "                    \n",
    "                    if SP.startswith('SP'):\n",
    "                        SP = 'Y'\n",
    "                    else:\n",
    "                        SP = 0\n",
    "            \n",
    "                    SP_dict[ID] = { 'SP': SP } \n",
    "        \n",
    "        return SP_dict\n",
    "    \n",
    "def parse_Predisi(file_name):\n",
    "    \n",
    "    with open (file_name, 'r') as fo:\n",
    "        \n",
    "        predisi_dict = dict()\n",
    "        header = True\n",
    "        \n",
    "        for line in fo:\n",
    "            if header:\n",
    "                header = False\n",
    "            else:\n",
    "                line = line.rstrip()\n",
    "                ID = line.split('\\t')[0].split('|')[1]\n",
    "                SP = line.split('\\t')[-2]\n",
    "                \n",
    "                predisi_dict[ID] = {'SP' : SP}\n",
    "                \n",
    "                \n",
    "    return predisi_dict\n",
    "    \n",
    "def parse_SPC(seq_dict, file_name):\n",
    "    \n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "            SPC_dict = dict()\n",
    "            header = True\n",
    "\n",
    "            for line in fo:\n",
    "                    if header:\n",
    "                        header = False\n",
    "                    else:\n",
    "                        line = line.rstrip()\n",
    "                        ID, SPC = line.split(',')[0], line.split(',')[1]\n",
    "\n",
    "                        SPC_dict[ID] = int(SPC)\n",
    "            \n",
    "            for ID in seq_dict:\n",
    "                \n",
    "                if SPC_dict.get(ID) == None:\n",
    "                    SPC_dict[ID] = 0\n",
    "            \n",
    "    return SPC_dict\n",
    "        \n",
    "        \n",
    "            \n",
    "def fasta_parser(fasta_filename):   \n",
    "    \n",
    "\n",
    "    fasta_fileobj = open(fasta_filename, 'r')\t## create a file obj from the specified file\n",
    "\n",
    "    sequence_name = ''\t\t\t\t## initialize strings to populate from file object info\n",
    "    sequence_desc = ''\n",
    "    sequence_string = ''\n",
    "    sequence_dict = {}\n",
    "\n",
    "    for line in fasta_fileobj:  \t\t\t## iterate through file object with for loop\n",
    "        line = line.rstrip()\t\t\t## strip white space on the right side (like a new line character!) \n",
    "\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(sequence_string) > 0:\n",
    "                sequence_dict[sequence_name] = sequence_string\t\n",
    "                sequence_string = ''  \t\t## reset for the new sequence\n",
    "            \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  ## split on only first space\n",
    "            sequence_name = sequence_info[0].split('|')[1]\n",
    "\t\n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "            else:\t\t\t\t\t## sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "\t\t\n",
    "           \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  \t## split on only first space\n",
    "           \n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "           \n",
    "            else:\n",
    "            # sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "             \n",
    "        else:\n",
    "            sequence_string += line  # incrementally elongate seq\n",
    "\n",
    "# When we reach the end of the FASTA file, we drop out of the\n",
    "# 'for' loop. However, we still have the last sequence record\n",
    "# stored in memory, which we haven't processed yet, because we\n",
    "# haven't observed a '>' symbol, so we must copy and paste any\n",
    "# code that we used to process sequences above to the code block\n",
    "# below. Check if sequence_string has a non-zero length to\n",
    "# determine whether to execute the sequence processing code:\n",
    "\n",
    "    if len(sequence_string) > 0:\n",
    "        sequence_dict[sequence_name] = sequence_string\n",
    "        \n",
    "    return sequence_dict\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contatins the definition of functions required \n",
    "\n",
    "import re\n",
    "\n",
    "## 2 missed cleavages are used for trypsinize, where the number of missed cleavages are recorded\n",
    "\n",
    "def trypsinize(prot_seq):\n",
    "    peptides= []\n",
    "    cut_sites=[0]\n",
    "    indices = []\n",
    "    pep = ''\n",
    "\n",
    "    for i in range(0,len(prot_seq)-1):\n",
    "        if prot_seq[i] == 'K' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        elif prot_seq[i] == 'R' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        \n",
    "    if cut_sites[-1]!=len(prot_seq):\n",
    "            cut_sites.append(len(prot_seq))\n",
    "            \n",
    "    if len(cut_sites)>2:\n",
    "            \n",
    "        for j in range(0,len(cut_sites)-3):\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+2]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+2]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "            indices = []\n",
    "\n",
    "            pep = prot_seq[cut_sites[j]:cut_sites[j+3]]\n",
    "            for i in range(cut_sites[j],cut_sites[j+3]):\n",
    "                indices.append(i+1)\n",
    "            peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 2})\n",
    "            indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-2]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-2]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-3]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-3],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 1})\n",
    "        indices = []\n",
    "\n",
    "        pep = prot_seq[cut_sites[-2]:cut_sites[-1]]\n",
    "        for i in range(cut_sites[-2],cut_sites[-1]):\n",
    "            indices.append(i+1)\n",
    "        peptides.append({'seq': pep,'indices': indices, 'missed_cleavages' : 0})\n",
    "        indices = []\n",
    "                    \n",
    "    else: #there is no trypsin site in the protein sequence\n",
    "        peptides.append({'seq' : prot_seq, 'indices' : range(1,len(prot_seq)+1), 'missed_cleavages' : 0})\n",
    "        \n",
    "    return peptides\n",
    "\n",
    "def ion_mim(prot_seq, charge_state):\n",
    "    \n",
    "    mass_table = {\n",
    "            \"A\" : 71.03711,\n",
    "            \"R\" : 156.10111,\n",
    "            \"N\" : 114.04293,\n",
    "            \"D\" : 115.02694,\n",
    "            \"C\" : 103.00919 + 57.02146,\n",
    "            \"E\" : 129.04259,\n",
    "            \"Q\" : 128.05858,\n",
    "            \"G\" : 57.02146,\n",
    "            \"H\" : 137.05891,\n",
    "            \"I\" : 113.08406,\n",
    "            \"L\" : 113.08406,\n",
    "            \"K\" : 128.09496,\n",
    "            \"M\" : 131.04049,\n",
    "            \"F\" : 147.06841,\n",
    "            \"P\" : 97.05276,\n",
    "            \"S\" : 87.03203,\n",
    "            \"T\" : 101.04768,\n",
    "            \"W\" : 186.07931,\n",
    "            \"Y\" : 163.06333,\n",
    "            \"V\" : 99.06841\n",
    "            }\n",
    "    \n",
    "    mass = 0\n",
    "    \n",
    "    for aa in mass_table:\n",
    "        mass += prot_seq.count(aa) * mass_table[aa]\n",
    "    \n",
    "    \n",
    "    ion_mass = mass + (charge_state * 1.007276)\n",
    "    m_z = ion_mass/charge_state\n",
    "    \n",
    "    return m_z\n",
    "\n",
    "\n",
    "def ok_for_MS(pep_list):\n",
    "    \n",
    "    for pep in pep_list:\n",
    "        \n",
    "        pep['okForMS'] = ''\n",
    "        \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 2) < 2000):\n",
    "            pep['okForMS'] += '2'\n",
    "            \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 3) < 2000):\n",
    "            pep['okForMS'] += ',3'\n",
    "        \n",
    "        if len(pep['seq']) > 5 and (ion_mim(pep['seq'], 4) < 2000):    \n",
    "            pep['okForMS'] += ',4'\n",
    "        \n",
    "        else:\n",
    "            pep['okForMS'] = None\n",
    "            \n",
    "    return pep_list      \n",
    "\n",
    "def make_prot_dict(seq_dict, tm_dict, phob_dict, sp_dict, predisi_dict, SPC_dict):\n",
    "    \n",
    "    prot_dict = dict()\n",
    "    \n",
    "    for ID in seq_dict:\n",
    "      \n",
    "      seq = seq_dict[ID]\n",
    "    \n",
    "      if tm_dict.get(ID) and phob_dict.get(ID) and sp_dict.get(ID) and predisi_dict.get(ID):\n",
    "        \n",
    "        \n",
    "        pep_list = trypsinize(seq)\n",
    "        \n",
    "        pep_list = ok_for_MS(pep_list)\n",
    "\n",
    "        \n",
    "        glyco_indices_S = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]S', seq ):\n",
    "            glyco_indices_S.append(match.start()+1)\n",
    "        \n",
    "        glyco_indices_T = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]T', seq ):\n",
    "            glyco_indices_T.append(match.start()+1)    \n",
    "            \n",
    "        glyco_indices_C = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]C', seq ):\n",
    "            glyco_indices_C.append(match.start()+1)   \n",
    "        \n",
    "        glyco_indices_V = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]V', seq ):\n",
    "            glyco_indices_V.append(match.start()+1)\n",
    "            \n",
    "        K_indices = list()\n",
    "        C_indices = list()\n",
    "        \n",
    "        for i in range(0, len(seq)):\n",
    "            if seq[i] == 'K':\n",
    "                K_indices.append(i+1)\n",
    "            elif seq[i] == 'C':\n",
    "                C_indices.append(i+1) \n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        prot_dict[ID] = {                             \n",
    "                            'seq_info' :\n",
    "                              { \n",
    "                                  'seq' : seq , \n",
    "                                  'seq_len' : len(seq) \n",
    "                              } , \n",
    "                             \n",
    "                            'topo' :\n",
    "                              {\n",
    "                                 'TMHMM' :\n",
    "                                    { 'inside' : tm_dict[ID]['inside']  ,\n",
    "                                      'outside' : tm_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : tm_dict[ID]['num_TM'] } ,\n",
    "                                 'Phobius' :\n",
    "                                    { 'inside' : phob_dict[ID]['inside']  , \n",
    "                                      'outside' : phob_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : phob_dict[ID]['num_TM'] }  \n",
    "                              } , \n",
    "                           \n",
    "                            'signal' : \n",
    "                              {\n",
    "                                  'Phobius' : phob_dict[ID]['SP'] ,\n",
    "                                  'SignalP' : sp_dict[ID]['SP'] , \n",
    "                                  'PrediSi' : predisi_dict[ID]['SP']  \n",
    "                              } , \n",
    "                            \n",
    "                            'SPC' : int(SPC_dict[ID]), \n",
    "            \n",
    "                            'peptides' : pep_list, \n",
    "\n",
    "            \n",
    "                            'motif_sites' : \n",
    "                               { \n",
    "                                 'NXS' :\n",
    "                                   { 'all' : glyco_indices_S , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXT' :\n",
    "                                   { 'all' : glyco_indices_T , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'NXC' :\n",
    "                                   { 'all' : glyco_indices_C , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'NXV' :\n",
    "                                   { 'all' : glyco_indices_V , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'C' :\n",
    "                                   { 'all' : C_indices , 'extracellular' : dict()  } , \n",
    "                                 \n",
    "                                 'K' :\n",
    "                                   { 'all' : K_indices , 'extracellular' : dict()  } , \n",
    "                                    \n",
    "                               }                          \n",
    "                                           \n",
    "                        }\n",
    "    print('step1_done')\n",
    "\n",
    "    \n",
    "    return prot_dict\n",
    "\n",
    "def EC_analysis(prot_dict, pred_method):\n",
    "    import time\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    motif_list = ['NXS','NXT','NXC','NXV','C','K'] \n",
    "        \n",
    "    for pred in pred_method: \n",
    "    \n",
    "        for ID, value in prot_dict.items():\n",
    "            count += 1\n",
    "\n",
    "            if count%200 == 0:\n",
    "                print('count is at',str(count))\n",
    "            \n",
    "            outside_indices = value['topo'][pred]['outside']\n",
    "            \n",
    "            if len(outside_indices) == 0:\n",
    "                for motif in motif_list:\n",
    "                    value['motif_sites'][motif]['extracellular'][pred] = []\n",
    "            \n",
    "            else: \n",
    "                for motif in motif_list:\n",
    "                    \n",
    "                    time.sleep(300)\n",
    "                    \n",
    "                    motif_indices = value['motif_sites'][motif]['all']\n",
    "                    out_motif = list( set(motif_indices) & set(outside_indices) )\n",
    "\n",
    "                    value['motif_sites'][motif]['extracellular'][pred] = out_motif \n",
    "\n",
    "                    for pep in value['peptides']:\n",
    "\n",
    "                        if pep['okForMS'] == None:\n",
    "                            pep[motif] = None\n",
    "\n",
    "                        else:                 \n",
    "                            pep[motif] = dict()\n",
    "                            pep[motif]['extracellular'] = dict()\n",
    "\n",
    "                            all = set(motif_indices) & set(pep['indices'])\n",
    "                            if all:    \n",
    "                                pep[motif]['all'] = {'num' : len(all), 'indices' : list(all) }\n",
    "\n",
    "                                out = set(out_motif) & set(pep['indices'])\n",
    "\n",
    "                                if out:\n",
    "                                    pep[motif]['extracellular'][pred] = {'num' : len(out), 'indices' : list(out)}\n",
    "\n",
    "    return prot_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequnces: 20416\n",
      "SignalP: 20413\n",
      "TMHMM: 20412\n",
      "Phobius: 20412\n",
      "SPC: 20424\n",
      "Predisi: 20416\n"
     ]
    }
   ],
   "source": [
    "file_name = 'can_uniprot-proteome_UP000005640+reviewed_yes.fasta'\n",
    "path = 'io_files/'\n",
    "\n",
    "seqs_dict = fasta_parser(path + file_name)\n",
    "print('sequnces:',len(seqs_dict))\n",
    "\n",
    "sp = parse_signalP(path + 'signalp_out.tsv')\n",
    "print('SignalP:',len(sp))\n",
    "\n",
    "TMHMM_dict  = parse_TMHMM(path + 'TMHMM_out_clean.tsv')\n",
    "print('TMHMM:',len(TMHMM_dict))\n",
    "\n",
    "Phobius_dict = parse_Phobius(path + 'Phobius_out_clean.tsv')\n",
    "print('Phobius:',len(Phobius_dict))\n",
    "\n",
    "SPC_dict = parse_SPC(seqs_dict, path + 'SPC_by_Source.csv')\n",
    "print('SPC:',len(SPC_dict))\n",
    "\n",
    "predisi_dict = parse_Predisi(path + 'predisi.txt')\n",
    "print('Predisi:',len(predisi_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1_done\n"
     ]
    }
   ],
   "source": [
    "prot_dict = make_prot_dict(seqs_dict, TMHMM_dict, Phobius_dict, sp, predisi_dict, SPC_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prot dict: 20405\n"
     ]
    }
   ],
   "source": [
    "print('Prot dict:', len(prot_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-61edaac3ed5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_methods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'TMHMM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Phobius'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mEC_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprot_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_methods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f8bcc23dea97>\u001b[0m in \u001b[0;36mEC_analysis\u001b[0;34m(prot_dict, pred_method)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmotif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmotif_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0mmotif_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'motif_sites'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmotif\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_methods = ['TMHMM','Phobius']\n",
    "\n",
    "EC_analysis(prot_dict, pred_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prot_dict['Q8N112'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for proteins of various num TM domains, report the ratios of EC residues to IC residues\n",
    "\n",
    "with open('TManalysis.tab', 'w') as fo:\n",
    "\n",
    "    header = 'Accession'\n",
    "    \n",
    "    for pred in ['TMHMM', 'Phobius', 'consensus', 'inclusive']:\n",
    "        header += '\\t' + pred+' #TM' +'\\t'+ pred+'I' +'\\t'+ pred+'O' +'\\t'+ pred+'Ratio(O/I)'\n",
    "        \n",
    "    fo.write(header+'\\n')\n",
    "    \n",
    "    for ID in prot_dict:\n",
    "        out = ''\n",
    "        out += ID\n",
    "        \n",
    "        prot = prot_dict[ID]\n",
    "                \n",
    "        for pred in ['TMHMM', 'Phobius', 'consensus', 'inclusive']:\n",
    "            num_TM = prot['topo'][pred]['num_TM']\n",
    "            I = len(prot['topo'][pred]['inside'])\n",
    "            O = len(prot['topo'][pred]['outside'])\n",
    "            \n",
    "            if I == 0:\n",
    "                ratio = 'inf'\n",
    "            else:\n",
    "                ratio = str(O/I)\n",
    "            \n",
    "            out += '\\t'+ str(num_TM) +'\\t'+ str(I) +'\\t'+ str(O) +'\\t'+ ratio\n",
    "            \n",
    "        fo.write(out +'\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each glycomotif record the number that are predicted extracellular, have SPC, have SPC\n",
    "\n",
    "counts = { \n",
    "    'no_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } , \n",
    "    'SPC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } ,\n",
    "    'SP_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'EC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SP-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-SP' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'all_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'total' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 }\n",
    "    }\n",
    "\n",
    "for ID in prot_dict :\n",
    "    prot = prot_dict[ID]\n",
    "    \n",
    "    SP = prot['signal']['inclusive']\n",
    "    SPC = \n",
    "    \n",
    "    for site in prot['glyco_sites']:\n",
    "        sites_dict = prot['glyco_sites'][site]\n",
    "        \n",
    "        all = set(sites_dict['all'])\n",
    "        inclusive = set(sites_dict['extracellular']['inclusive'])\n",
    "        num_ic = len(all) - len(inclusive)\n",
    "    \n",
    "        counts['total'][site] += len(all)\n",
    "        \n",
    "        if SP == 'Y' and SPC > 0:\n",
    "            counts['all_info'][site] += len(inclusive)\n",
    "            counts['SPC-SP'][site] += num_ic\n",
    "            \n",
    "        elif SP == 'Y' and SPC == 0:\n",
    "            counts['SP-EC'][site] += len(inclusive)\n",
    "            counts['SP_only'][site] += num_ic\n",
    "            \n",
    "        elif SP != 'Y' and SPC > 0:\n",
    "            counts['SPC-EC'][site] += len(inclusive)\n",
    "            counts['SPC_only'][site] += num_ic\n",
    "            \n",
    "        else:\n",
    "            counts['EC_only'][site] += len(inclusive)\n",
    "            counts['no_info'][site] += num_ic\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## for each accession --> report SPC, signal, and TM --> report number of glycopeptide, number of EC-glycopeptides \n",
    "\n",
    "with open('glycopep.tab', 'w') as fo:\n",
    "\n",
    "    header = 'Accession\\tSPC\\tspInc\\tspCon\\tspTot\\ttmInc\\ttmCon'\n",
    "    for aa in ['S', 'T', 'C', 'V']:\n",
    "        header += '\\t'+ aa+'gp' +'\\t'+ aa+'gpInc' +'\\t'+ aa+'gpCon'\n",
    "    fo.write(header+'\\n')\n",
    "    \n",
    "    for ID in prot_dict:\n",
    "        out = ''\n",
    "                \n",
    "        prot = prot_dict[ID]\n",
    "        SPC = str(prot['SPC'])\n",
    "        spInc = prot['signal']['inclusive'] \n",
    "        spCon = prot['signal']['consensus']\n",
    "        spTot = 0\n",
    "        \n",
    "        if prot['signal']['Phobius']  == 'Y':\n",
    "            spTot += 1\n",
    "        if prot['signal']['PrediSi']  == 'Y':\n",
    "            spTot += 1\n",
    "        if prot['signal']['SignalP']  == 'Y':\n",
    "            spTot += 1            \n",
    "            \n",
    "        tmInc = str(prot['topo']['inclusive']['num_TM'])\n",
    "        tmCon = str(prot['topo']['consensus']['num_TM'])\n",
    "        \n",
    "        out += ID +'\\t'+ SPC +'\\t'+ spInc +'\\t'+ spCon +'\\t'+ str(spTot) +'\\t'+ tmInc +'\\t'+ tmCon \n",
    "        \n",
    "        for aa in ['S', 'T', 'C', 'V']:\n",
    "            gp = len(prot['glycopeptides'][aa]['2']['all'])\n",
    "            ECgpInc =  len(prot['glycopeptides'][aa]['2']['extracellular']['inclusive'])\n",
    "            ECgpCon =  len(prot['glycopeptides'][aa]['2']['extracellular']['consensus'])\n",
    "            \n",
    "            out += '\\t' + str(gp) +'\\t'+ str(ECgpInc) +'\\t'+ str(ECgpCon) \n",
    "            \n",
    "        fo.write(out + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prot_dict_new.out', 'w') as fo:\n",
    "    fo.write(str(prot_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
