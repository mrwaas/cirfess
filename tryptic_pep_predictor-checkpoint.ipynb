{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Motif Coverage Predictor\n",
    "\n",
    "This program is designed to return the number of peptides for a protein (or set of proteins) which contain a given sequence motif (N-glyco motif) and are suitable for mass spectrometry. \n",
    "\n",
    "The program considers protein-level information from prediction tools such as `TMHMM`, `Phobius`, and `SignalP` in order to better interpret the context of possible sequence motifs. \n",
    "\n",
    "## I/O and Usage\n",
    "\n",
    "### Usage\n",
    "\n",
    "### Input\n",
    "\n",
    "\n",
    "### Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the definitions of functions to parse the various input file types, including:\n",
    "    # TMHMM short format output\n",
    "    # Phobius short format output\n",
    "    # Signal P short format output\n",
    "    # Fasta file\n",
    "\n",
    "# \n",
    "\n",
    "# Dependencies\n",
    "    # regular expressions (re)\n",
    "\n",
    "\n",
    "\n",
    "def parse_TMHMM(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        tm_dict = dict()\n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID = line.split('\\t')[0]\n",
    "            length = int(line.split('\\t')[1].split('=')[1])\n",
    "            topo_str = line.split('\\t')[5].split('=')[1]\n",
    "            num_TM = int(line.split('\\t')[4].split('=')[1])\n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : None }\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'num_TM' : num_TM , 'topo' : topo_str } \n",
    "                \n",
    "        \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                   i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                   o1.append(1)\n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "                    \n",
    "    return tm_dict\n",
    "\n",
    "def parse_Phobius(file_name):\n",
    "    import re\n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "        tm_dict = dict()\n",
    "        \n",
    "        with open('io_files/human_plus_leftovers.tab' , 'r') as fo2:\n",
    "            length_dict = dict()\n",
    "            header = True\n",
    "            for line in fo2:\n",
    "                if header:\n",
    "                    header = False\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    length = line.split('\\t')[6]\n",
    "                    length_dict[ID] = int(length)\n",
    "                            \n",
    "        for line in fo:\n",
    "            line = line.rstrip()\n",
    "            ID, num_TM, SP, topo_str = line.split()\n",
    "            ID = ID.split('|')[1]\n",
    "            \n",
    "            if bool(length_dict.get(ID)):\n",
    "                length = length_dict[ID]\n",
    "            else:\n",
    "                continue   \n",
    "            \n",
    "            if len(topo_str) == 1:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : None , \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "            else:\n",
    "                tm_dict[ID] = { 'length' : length, 'topo' : topo_str ,  \\\n",
    "                                'num_TM' : int(num_TM), 'SP' : SP}\n",
    "                \n",
    "        for key in tm_dict:\n",
    "            inside = list()\n",
    "            outside = list()\n",
    "            i1 = list()\n",
    "            i2 = list()\n",
    "            o1 = list()\n",
    "            o2 = list()\n",
    "            \n",
    "            topo = tm_dict[key]['topo']\n",
    "            \n",
    "            if topo:\n",
    "                \n",
    "                if topo.startswith('i'):\n",
    "                    i1.append(1)\n",
    "                elif topo.startswith('o'):\n",
    "                    o1.append(1)\n",
    "                else:\n",
    "                    match = re.search(r'\\S+/(\\S+?)([io])(\\S*)', topo)\n",
    "                    if match:\n",
    "                        topo = match.group(2) + match.group(3)\n",
    "                    else:\n",
    "                        print(topo)\n",
    "                    if topo.startswith('i'):\n",
    "                        i1.append(int(match.group(1)))\n",
    "                    elif topo.startswith('o'):\n",
    "                        o1.append(int(match.group(1)))\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                for match in re.finditer(r'i(\\d+?)-(\\d+?)o', topo):\n",
    "                    i2.append(int(match.group(1))-1)\n",
    "                    o1.append(int(match.group(2))+1)\n",
    "                for match in re.finditer(r'o(\\d+?)-(\\d+?)i', topo):\n",
    "                    i1.append(int(match.group(2))+1)\n",
    "                    o2.append(int(match.group(1))-1)\n",
    "        \n",
    "                if len(i1) > len(i2):\n",
    "                    i2.append(tm_dict[key]['length'])\n",
    "                elif len(o1) > len(o2):\n",
    "                    o2.append(tm_dict[key]['length'])\n",
    "             \n",
    "            inside_residues = list()\n",
    "            outside_residues = list()\n",
    "            \n",
    "            for i in range(len(i1)):\n",
    "                inside_residues.extend(list(range(i1[i],i2[i]+1)))\n",
    "                \n",
    "            for i in range(len(o1)):\n",
    "                outside_residues.extend(list(range(o1[i],o2[i]+1)))\n",
    "                \n",
    "            tm_dict[key]['inside'] = inside_residues\n",
    "            tm_dict[key]['outside'] = outside_residues\n",
    "            \n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def parse_signalP(file_name):\n",
    "    \n",
    "        with open(file_name, 'r') as fo:\n",
    "            \n",
    "            SP_dict = dict()\n",
    "            \n",
    "            for line in fo:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line.rstrip()\n",
    "                    ID = line.split()[0]\n",
    "                    SP = line.split()[1]\n",
    "                    \n",
    "                    if SP.startswith('SP'):\n",
    "                        SP = 'Y'\n",
    "                    else:\n",
    "                        SP = 0\n",
    "            \n",
    "                    SP_dict[ID] = { 'SP': SP } \n",
    "        \n",
    "        return SP_dict\n",
    "    \n",
    "def parse_Predisi(file_name):\n",
    "    \n",
    "    with open (file_name, 'r') as fo:\n",
    "        \n",
    "        predisi_dict = dict()\n",
    "        header = True\n",
    "        \n",
    "        for line in fo:\n",
    "            if header:\n",
    "                header = False\n",
    "            else:\n",
    "                line = line.rstrip()\n",
    "                ID = line.split('\\t')[0].split('|')[1]\n",
    "                SP = line.split('\\t')[-2]\n",
    "                \n",
    "                predisi_dict[ID] = {'SP' : SP}\n",
    "                \n",
    "                \n",
    "    return predisi_dict\n",
    "    \n",
    "def parse_SPC(seq_dict, file_name):\n",
    "    \n",
    "    with open(file_name, 'r') as fo:\n",
    "        \n",
    "            SPC_dict = dict()\n",
    "            header = True\n",
    "\n",
    "            for line in fo:\n",
    "                    if header:\n",
    "                        header = False\n",
    "                    else:\n",
    "                        line = line.rstrip()\n",
    "                        ID, SPC = line.split(',')[0], line.split(',')[1]\n",
    "\n",
    "                        SPC_dict[ID] = int(SPC)\n",
    "            \n",
    "            for ID in seq_dict:\n",
    "                \n",
    "                if SPC_dict.get(ID) == None:\n",
    "                    SPC_dict[ID] = 0\n",
    "            \n",
    "    return SPC_dict\n",
    "        \n",
    "        \n",
    "            \n",
    "def fasta_parser(fasta_filename):   \n",
    "    \n",
    "\n",
    "    fasta_fileobj = open(fasta_filename, 'r')\t## create a file obj from the specified file\n",
    "\n",
    "    sequence_name = ''\t\t\t\t## initialize strings to populate from file object info\n",
    "    sequence_desc = ''\n",
    "    sequence_string = ''\n",
    "    sequence_dict = {}\n",
    "\n",
    "    for line in fasta_fileobj:  \t\t\t## iterate through file object with for loop\n",
    "        line = line.rstrip()\t\t\t## strip white space on the right side (like a new line character!) \n",
    "\n",
    "        if line.startswith('>'):\n",
    "            \n",
    "            if len(sequence_string) > 0:\n",
    "                sequence_dict[sequence_name] = sequence_string\t\n",
    "                sequence_string = ''  \t\t## reset for the new sequence\n",
    "            \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  ## split on only first space\n",
    "            sequence_name = sequence_info[0].split('|')[1]\n",
    "\t\n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "            else:\t\t\t\t\t## sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "\t\t\n",
    "           \n",
    "            line = line.lstrip('>')  \t\t## remove leading `>` char\n",
    "            sequence_info = line.split(maxsplit=1)  \t## split on only first space\n",
    "           \n",
    "            if len(sequence_info) > 1:\n",
    "                sequence_desc = sequence_info[1]\n",
    "           \n",
    "            else:\n",
    "            # sequence has no description, set to empty\n",
    "                sequence_desc = ''\n",
    "             \n",
    "        else:\n",
    "            sequence_string += line  # incrementally elongate seq\n",
    "\n",
    "# When we reach the end of the FASTA file, we drop out of the\n",
    "# 'for' loop. However, we still have the last sequence record\n",
    "# stored in memory, which we haven't processed yet, because we\n",
    "# haven't observed a '>' symbol, so we must copy and paste any\n",
    "# code that we used to process sequences above to the code block\n",
    "# below. Check if sequence_string has a non-zero length to\n",
    "# determine whether to execute the sequence processing code:\n",
    "\n",
    "    if len(sequence_string) > 0:\n",
    "        sequence_dict[sequence_name] = sequence_string\n",
    "        \n",
    "    return sequence_dict\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contatins the definition of functions required \n",
    "\n",
    "import re\n",
    "\n",
    "def trypsinize(prot_seq, miss_cleavage):\n",
    "    peptides= []\n",
    "    cut_sites=[0]\n",
    "    indices = []\n",
    "    pep = ''\n",
    "\n",
    "    for i in range(0,len(prot_seq)-1):\n",
    "        if prot_seq[i] == 'K' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        elif prot_seq[i] == 'R' and prot_seq[i+1] != 'P':\n",
    "            cut_sites.append(i+1)\n",
    "        \n",
    "    if cut_sites[-1]!=len(prot_seq):\n",
    "            cut_sites.append(len(prot_seq))\n",
    "            \n",
    "    if len(cut_sites)>2:\n",
    "            if  miss_cleavage==0:\n",
    "                for j in range(0,len(cut_sites)-1):\n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                    \n",
    "            elif miss_cleavage==1:\n",
    "                for j in range(0,len(cut_sites)-2):\n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                    \n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+2]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+2]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                \n",
    "                pep = prot_seq[cut_sites[-2]:cut_sites[-1]]\n",
    "                for i in range(cut_sites[-2],cut_sites[-1]):\n",
    "                    indices.append(i+1)\n",
    "                peptides.append({'seq': pep,'indices': indices})\n",
    "                indices = []\n",
    "    \n",
    "            elif miss_cleavage==2:\n",
    "                for j in range(0,len(cut_sites)-3):\n",
    "                    \n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+1]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+1]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                \n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+2]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+2]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                    \n",
    "                    pep = prot_seq[cut_sites[j]:cut_sites[j+3]]\n",
    "                    for i in range(cut_sites[j],cut_sites[j+3]):\n",
    "                        indices.append(i+1)\n",
    "                    peptides.append({'seq': pep,'indices': indices})\n",
    "                    indices = []\n",
    "                \n",
    "                pep = prot_seq[cut_sites[-3]:cut_sites[-2]]\n",
    "                for i in range(cut_sites[-3],cut_sites[-2]):\n",
    "                    indices.append(i+1)\n",
    "                peptides.append({'seq': pep,'indices': indices})\n",
    "                indices = []\n",
    "                    \n",
    "                pep = prot_seq[cut_sites[-3]:cut_sites[-1]]\n",
    "                for i in range(cut_sites[-3],cut_sites[-1]):\n",
    "                    indices.append(i+1)\n",
    "                peptides.append({'seq': pep,'indices': indices})\n",
    "                indices = []\n",
    "                    \n",
    "                pep = prot_seq[cut_sites[-2]:cut_sites[-1]]\n",
    "                for i in range(cut_sites[-2],cut_sites[-1]):\n",
    "                    indices.append(i+1)\n",
    "                peptides.append({'seq': pep,'indices': indices})\n",
    "                indices = []\n",
    "                    \n",
    "    else: #there is no trypsin site in the protein sequence\n",
    "        peptides.append({'seq' : prot_seq, 'indices' : range(1,len(prot_seq)+1)})\n",
    "    return peptides\n",
    "\n",
    "def ion_mim(prot_seq, charge_state):\n",
    "    \n",
    "    mass_table = {\n",
    "            \"A\" : 71.03711,\n",
    "            \"R\" : 156.10111,\n",
    "            \"N\" : 114.04293,\n",
    "            \"D\" : 115.02694,\n",
    "            \"C\" : 103.00919 + 57.02146,\n",
    "            \"E\" : 129.04259,\n",
    "            \"Q\" : 128.05858,\n",
    "            \"G\" : 57.02146,\n",
    "            \"H\" : 137.05891,\n",
    "            \"I\" : 113.08406,\n",
    "            \"L\" : 113.08406,\n",
    "            \"K\" : 128.09496,\n",
    "            \"M\" : 131.04049,\n",
    "            \"F\" : 147.06841,\n",
    "            \"P\" : 97.05276,\n",
    "            \"S\" : 87.03203,\n",
    "            \"T\" : 101.04768,\n",
    "            \"W\" : 186.07931,\n",
    "            \"Y\" : 163.06333,\n",
    "            \"V\" : 99.06841\n",
    "            }\n",
    "    \n",
    "    mass = 0\n",
    "    \n",
    "    for aa in mass_table:\n",
    "        mass += prot_seq.count(aa) * mass_table[aa]\n",
    "    \n",
    "    \n",
    "    ion_mass = mass + (charge_state * 1.007276)\n",
    "    m_z = ion_mass/charge_state\n",
    "    \n",
    "    return m_z\n",
    "\n",
    "\n",
    "def ok_for_MS(pep_list, charge):\n",
    "    \n",
    "    filtered_list = []\n",
    "    \n",
    "    for item in pep_list:\n",
    "        if len(item['seq']) > 5 and (ion_mim(item['seq'], charge) < 2000):\n",
    "            filtered_list.append(item)\n",
    "            \n",
    "    return filtered_list       \n",
    "\n",
    "def make_prot_dict(seq_dict, tm_dict, phob_dict, sp_dict, predisi_dict, SPC_dict):\n",
    "    \n",
    "    prot_dict = dict()\n",
    "    \n",
    "    for ID in seq_dict:\n",
    "      \n",
    "      seq = seq_dict[ID]\n",
    "    \n",
    "      if tm_dict.get(ID) and phob_dict.get(ID) and sp_dict.get(ID) and predisi_dict.get(ID) and SPC_dict.get(ID):\n",
    "        \n",
    "        con_inside = list( set(phob_dict[ID]['inside']) & set(tm_dict[ID]['inside']) )\n",
    "        con_outside = list( set(phob_dict[ID]['outside']) & set(tm_dict[ID]['outside']) )\n",
    "        con_numTM = min(phob_dict[ID]['num_TM'], tm_dict[ID]['num_TM'])\n",
    "          \n",
    "        inc_inside = list( set(phob_dict[ID]['inside']) | set(tm_dict[ID]['inside']) )\n",
    "        inc_outside = list( set(phob_dict[ID]['outside']) | set(tm_dict[ID]['outside']) )\n",
    "        inc_numTM = max(phob_dict[ID]['num_TM'], tm_dict[ID]['num_TM'])  \n",
    "        \n",
    "        if phob_dict[ID]['SP'] == 'Y' and  sp_dict[ID]['SP'] == 'Y' and predisi_dict[ID]['SP'] == 'Y' :\n",
    "            con_sp = 'Y'\n",
    "            inc_sp = 'Y'\n",
    "        elif phob_dict[ID]['SP'] == 'Y' or  sp_dict[ID]['SP'] == 'Y' or predisi_dict[ID]['SP'] == 'Y' : \n",
    "            con_sp = 'N'\n",
    "            inc_sp = 'Y'\n",
    "        else :\n",
    "            con_sp = 'N'\n",
    "            inc_sp = 'N'\n",
    "            \n",
    "        pep0 = trypsinize(seq, 0)\n",
    "        pep1 = trypsinize(seq, 1)\n",
    "        pep2 = trypsinize(seq, 2)\n",
    "        \n",
    "        ok0 = ok_for_MS(pep0, 2)                 \n",
    "        ok1 = ok_for_MS(pep1, 2)\n",
    "        ok2 = ok_for_MS(pep2, 2)\n",
    "        \n",
    "        glyco_indices_S = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]S', seq ):\n",
    "            glyco_indices_S.append(match.start()+1)\n",
    "        \n",
    "        glyco_indices_T = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]T', seq ):\n",
    "            glyco_indices_T.append(match.start()+1)    \n",
    "            \n",
    "        glyco_indices_C = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]C', seq ):\n",
    "            glyco_indices_C.append(match.start()+1)   \n",
    "        \n",
    "        glyco_indices_V = list()\n",
    "        \n",
    "        for match in re.finditer(r'N[^P]V', seq ):\n",
    "            glyco_indices_V.append(match.start()+1)\n",
    "            \n",
    "        K_indices = list()\n",
    "        C_indices = list()\n",
    "        \n",
    "        for i in range(0, len(seq)):\n",
    "            if seq[i] == 'K':\n",
    "                K_indices.append(i+1)\n",
    "            elif seq[i] == 'C':\n",
    "                C_indices.append(i+1)        \n",
    "        \n",
    "        prot_dict[ID] = {                             \n",
    "                            'seq_info' :\n",
    "                              { \n",
    "                                  'seq' : seq , \n",
    "                                  'seq_len' : len(seq) \n",
    "                              } , \n",
    "                             \n",
    "                            'topo' :\n",
    "                              {\n",
    "                                 'TMHMM' :\n",
    "                                    { 'inside' : tm_dict[ID]['inside']  ,\n",
    "                                      'outside' : tm_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : tm_dict[ID]['num_TM'] } ,\n",
    "                                 'Phobius' :\n",
    "                                    { 'inside' : phob_dict[ID]['inside']  , \n",
    "                                      'outside' : phob_dict[ID]['outside']  ,\n",
    "                                      'num_TM' : phob_dict[ID]['num_TM'] } ,\n",
    "                                 'concensus' :\n",
    "                                    { 'inside' : con_inside  ,\n",
    "                                      'outside' : con_outside  ,\n",
    "                                      'num_TM' : con_numTM } ,\n",
    "                                 'inclusive' :\n",
    "                                    { 'inside' : inc_inside  ,\n",
    "                                      'outside' : inc_outside  , \n",
    "                                      'num_TM' : inc_numTM }  \n",
    "                              } , \n",
    "                           \n",
    "                            'signal' : \n",
    "                              {\n",
    "                                  'Phobius' : phob_dict[ID]['SP'] ,\n",
    "                                  'SignalP' : sp_dict[ID]['SP'] , \n",
    "                                  'PrediSi' : predisi_dict[ID]['SP'] , \n",
    "                                  'concensus' :  con_sp ,\n",
    "                                  'inclusive' :  inc_sp \n",
    "                              } , \n",
    "                            \n",
    "                            'SPC' : int(SPC_dict[ID]), \n",
    "            \n",
    "                            'peptides' : \n",
    "                               {\n",
    "                                   '0' : pep0 ,\n",
    "                                   '1' : pep1 ,\n",
    "                                   '2' : pep2 \n",
    "                               } ,\n",
    "            \n",
    "                            'filtered_peptides' :\n",
    "                               {\n",
    "                                   '0' : ok0 , \n",
    "                                   '1' : ok1 ,\n",
    "                                   '2' : ok2 \n",
    "                               } ,\n",
    "            \n",
    "                            'glyco_sites' : \n",
    "                               { \n",
    "                                 'S' :\n",
    "                                   { 'all' : glyco_indices_S , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'T' :\n",
    "                                   { 'all' : glyco_indices_T , 'extracellular' : dict()  } , \n",
    "                                   \n",
    "                                 'C' :\n",
    "                                   { 'all' : glyco_indices_C , 'extracellular' : dict()  } ,\n",
    "                                   \n",
    "                                 'V' :\n",
    "                                   { 'all' : glyco_indices_V , 'extracellular' : dict()  }                                   \n",
    "                               }  ,\n",
    "            \n",
    "                            'glycopeptides' :\n",
    "                                {\n",
    "                                    'S' :\n",
    "                                        { '0': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '1': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '2': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } \n",
    "                                        },\n",
    "                                    \n",
    "                                    'C' :\n",
    "                                        { '0': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '1': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '2': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } \n",
    "                                        },                                    \n",
    "                                    \n",
    "                                    'T' :\n",
    "                                        { '0': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '1': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '2': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } \n",
    "                                        },\n",
    "                                    \n",
    "                                    'V' :\n",
    "                                        { '0': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '1': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } ,\n",
    "                                          '2': \n",
    "                                            { 'all' : '' , 'extracellular' : dict() } \n",
    "                                        }                                    \n",
    "                                }\n",
    "                         \n",
    "            \n",
    "                        }\n",
    "    return prot_dict\n",
    "\n",
    "\n",
    "def EC_glyco(prot_dict, pred_method):\n",
    "    \n",
    "    for ID, value in prot_dict.items():\n",
    "        \n",
    "        outside_indices = value['topo'][pred_method]['outside']\n",
    "        \n",
    "        for aa in ['S', 'T', 'C', 'V']:\n",
    "            \n",
    "            glyco_indices = value['glyco_sites'][aa]['all']\n",
    "            \n",
    "            out_glyco = list( set(glyco_indices) & set(outside_indices) )\n",
    "            \n",
    "            value['glyco_sites'][aa]['extracellular'][pred_method] = out_glyco \n",
    "        \n",
    "            for i in ['0', '1', '2']:\n",
    "\n",
    "                glyco_peps = []     \n",
    "                EC_glyco_peps = []\n",
    "                \n",
    "                for pep in value['filtered_peptides'][i]:\n",
    "                    \n",
    "                    if set(glyco_indices) & set(pep['indices']):\n",
    "                        \n",
    "                        glyco_peps.append(pep)\n",
    "                        \n",
    "                        if set(out_glyco) & set(pep['indices']):\n",
    "                            EC_glyco_peps.append(pep)\n",
    "                            \n",
    "                value['glycopeptides'][aa][i]['all'] = glyco_peps\n",
    "                value['glycopeptides'][aa][i]['extracellular'][pred_method] = EC_glyco_peps\n",
    " \n",
    "    return prot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'can_uniprot-proteome_UP000005640+reviewed_yes.fasta'\n",
    "path = 'io_files/'\n",
    "\n",
    "seqs_dict = fasta_parser(path + file_name)\n",
    "sp = parse_signalP(path + 'signalp_out.tsv')      \n",
    "TMHMM_dict  = parse_TMHMM(path + 'TMHMM_out_clean.tsv')\n",
    "Phobius_dict = parse_Phobius(path + 'Phobius_out_clean.tsv')\n",
    "SPC_dict = parse_SPC(seqs_dict, path + 'SPC_by_Source.csv')\n",
    "predisi_dict = parse_Predisi(path + 'predisi.txt')\n",
    "\n",
    "prot_dict = make_prot_dict(seqs_dict, TMHMM_dict, Phobius_dict, sp, predisi_dict, SPC_dict)\n",
    "\n",
    "for pred in ['TMHMM', 'Phobius', 'concensus', 'inclusive']:\n",
    "    prot_dict = EC_glyco(prot_dict, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequnces: 20416\n",
      "SignalP: 20413\n",
      "TMHMM: 20412\n",
      "Phobius: 20412\n",
      "SPC: 20424\n",
      "Predisi: 20416\n",
      "Prot dict: 5397\n"
     ]
    }
   ],
   "source": [
    "print('sequnces:',len(seqs_dict))\n",
    "print('SignalP:',len(sp))\n",
    "print('TMHMM:',len(TMHMM_dict))\n",
    "print('Phobius:',len(Phobius_dict))\n",
    "print('SPC:',len(SPC_dict))\n",
    "print('Predisi:',len(predisi_dict))\n",
    "\n",
    "print('Prot dict:', len(prot_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each glycomotif record the number that are predicted extracellular, have SPC, have SPC\n",
    "\n",
    "counts = { \n",
    "    'no_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } , \n",
    "    'SPC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 } ,\n",
    "    'SP_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'EC_only' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SP-EC' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'SPC-SP' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'all_info' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 },\n",
    "    'total' : { 'S' : 0 , 'C' : 0 , 'T' : 0 , 'V' : 0 }\n",
    "    }\n",
    "\n",
    "for ID in prot_dict :\n",
    "    prot = prot_dict[ID]\n",
    "    \n",
    "    SP = prot['signal']['inclusive']\n",
    "    SPC = \n",
    "    \n",
    "    for site in prot['glyco_sites']:\n",
    "        sites_dict = prot['glyco_sites'][site]\n",
    "        \n",
    "        all = set(sites_dict['all'])\n",
    "        inclusive = set(sites_dict['extracellular']['inclusive'])\n",
    "        num_ic = len(all) - len(inclusive)\n",
    "    \n",
    "        counts['total'][site] += len(all)\n",
    "        \n",
    "        if SP == 'Y' and SPC > 0:\n",
    "            counts['all_info'][site] += len(inclusive)\n",
    "            counts['SPC-SP'][site] += num_ic\n",
    "            \n",
    "        elif SP == 'Y' and SPC == 0:\n",
    "            counts['SP-EC'][site] += len(inclusive)\n",
    "            counts['SP_only'][site] += num_ic\n",
    "            \n",
    "        elif SP != 'Y' and SPC > 0:\n",
    "            counts['SPC-EC'][site] += len(inclusive)\n",
    "            counts['SPC_only'][site] += num_ic\n",
    "            \n",
    "        else:\n",
    "            counts['EC_only'][site] += len(inclusive)\n",
    "            counts['no_info'][site] += num_ic\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
